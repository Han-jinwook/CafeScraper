#!/usr/bin/env python3
"""
ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ë° JavaScript ë¡œë”© ëŒ€ê¸° í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def test_login_and_wait():
    """ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ë° JavaScript ë¡œë”© ëŒ€ê¸°"""
    
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™í•˜ì—¬ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        print("ğŸŒ ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ ì ‘ì†...")
        scraper.driver.get("https://www.naver.com")
        time.sleep(3)
        
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        print("ğŸ” ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì¤‘...")
        login_indicators = [
            len(scraper.driver.find_elements("xpath", "//a[contains(text(), 'ë¡œê·¸ì¸')]")) == 0,
            len(scraper.driver.find_elements("xpath", "//a[contains(@href, 'nid.naver.com')]")) > 0,
            len(scraper.driver.find_elements("xpath", "//span[contains(@class, 'MyView-module__link_login___HpHMW')]")) > 0,
            len(scraper.driver.find_elements("xpath", "//a[contains(text(), 'ë¡œê·¸ì•„ì›ƒ')]")) > 0
        ]
        
        if any(login_indicators):
            print("âœ… ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ë¨")
        else:
            print("âŒ ë¡œê·¸ì¸ë˜ì§€ ì•ŠìŒ")
            return
        
        # ì ‘ê·¼ ê°€ëŠ¥í•œ ì¹´í˜ ì°¾ê¸° (ì¼ë°˜ì ì¸ ê³µê°œ ì¹´í˜)
        test_cafes = [
            "https://cafe.naver.com/joonggonara",  # ì¤‘ê³ ë‚˜ë¼
            "https://cafe.naver.com/steamindiegame",  # ìŠ¤íŒ€ ì¸ë””ê²Œì„
            "https://cafe.naver.com/steamindiegame/1",  # ìŠ¤íŒ€ ì¸ë””ê²Œì„ ê²Œì‹œíŒ
        ]
        
        for cafe_url in test_cafes:
            print(f"\nğŸŒ ì¹´í˜ ì ‘ì† ì‹œë„: {cafe_url}")
            try:
                scraper.driver.get(cafe_url)
                time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                
                # í˜ì´ì§€ ì œëª© í™•ì¸
                page_title = scraper.driver.title
                print(f"   í˜ì´ì§€ ì œëª©: {page_title}")
                
                # ì ‘ê·¼ ê°€ëŠ¥í•œì§€ í™•ì¸
                if "ë©¤ë²„ê°€ ì•„ë‹™ë‹ˆë‹¤" in scraper.driver.page_source:
                    print("   âŒ ë©¤ë²„ê°€ ì•„ë‹˜")
                    continue
                elif "ë¡œê·¸ì¸" in page_title or "ë„¤ì´ë²„ ì¹´í˜" in page_title:
                    print("   âœ… ì ‘ê·¼ ê°€ëŠ¥")
                    
                    # JavaScript ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
                    print("   â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
                    time.sleep(30)
                    
                    # ë‹¤ì‹œ í˜ì´ì§€ ì†ŒìŠ¤ í™•ì¸
                    page_source = scraper.driver.page_source
                    
                    # h íƒœê·¸ë“¤ ë‹¤ì‹œ í™•ì¸
                    h_elements = scraper.driver.find_elements("css selector", "h1, h2, h3, h4, h5, h6")
                    print(f"   ğŸ“„ h íƒœê·¸ ë°œê²¬: {len(h_elements)}ê°œ")
                    
                    for i, elem in enumerate(h_elements[:5]):
                        try:
                            text = elem.text.strip()
                            if text:
                                print(f"      {i+1}. {text[:50]}...")
                        except:
                            pass
                    
                    # se- í´ë˜ìŠ¤ë“¤ í™•ì¸
                    se_elements = scraper.driver.find_elements("css selector", "[class*='se-']")
                    print(f"   ğŸ“ se- í´ë˜ìŠ¤ ë°œê²¬: {len(se_elements)}ê°œ")
                    
                    # content ê´€ë ¨ í´ë˜ìŠ¤ë“¤ í™•ì¸
                    content_elements = scraper.driver.find_elements("css selector", "[class*='content']")
                    print(f"   ğŸ“„ content í´ë˜ìŠ¤ ë°œê²¬: {len(content_elements)}ê°œ")
                    
                    if len(h_elements) > 0 or len(se_elements) > 0 or len(content_elements) > 0:
                        print("   âœ… ê²Œì‹œê¸€ ë‚´ìš© ë°œê²¬!")
                        
                        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                        scraper.driver.save_screenshot(f"debug_successful_page_{cafe_url.split('/')[-1]}.png")
                        
                        # HTML ì €ì¥
                        with open(f"debug_successful_page_{cafe_url.split('/')[-1]}.html", "w", encoding="utf-8") as f:
                            f.write(page_source)
                        
                        print(f"   ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ë° HTML ì €ì¥ ì™„ë£Œ")
                        break
                    else:
                        print("   âŒ ê²Œì‹œê¸€ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
            except Exception as e:
                print(f"   âŒ ì˜¤ë¥˜: {e}")
                continue
        
        print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    test_login_and_wait()
