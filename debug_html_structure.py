#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ HTML êµ¬ì¡° ë¶„ì„ ìŠ¤í¬ë¦½íŠ¸
ì‹¤ì œ ê²Œì‹œê¸€ í˜ì´ì§€ì˜ HTML êµ¬ì¡°ë¥¼ íŒŒì•…í•˜ì—¬ ìƒˆë¡œìš´ ì…€ë ‰í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def analyze_html_structure():
    """ì‹¤ì œ ê²Œì‹œê¸€ í˜ì´ì§€ì˜ HTML êµ¬ì¡°ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤."""
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL (ì‹¤ì œ ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€)
        test_url = "https://cafe.naver.com/vitamind/1758864410"  # ì‹¤ì œ ê²Œì‹œê¸€ URL
        
        print(f"ğŸŒ ê²Œì‹œê¸€ í˜ì´ì§€ ì ‘ì†: {test_url}")
        scraper.driver.get(test_url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        scraper.driver.save_screenshot("debug_real_structure.png")
        print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_real_structure.png")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ë¶„ì„
        print("ğŸ” í˜ì´ì§€ ì†ŒìŠ¤ ë¶„ì„ ì¤‘...")
        page_source = scraper.driver.page_source
        
        # HTML íŒŒì¼ë¡œ ì €ì¥
        with open("debug_page_source_full.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("ğŸ’¾ ì „ì²´ HTML ì†ŒìŠ¤ ì €ì¥: debug_page_source_full.html")
        
        # ì œëª© ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ” ì œëª© ê´€ë ¨ ìš”ì†Œ ë¶„ì„:")
        title_elements = scraper.driver.find_elements("css selector", "h1, h2, h3, h4, h5, h6")
        print(f"   ë°œê²¬ëœ h íƒœê·¸: {len(title_elements)}ê°œ")
        
        for i, elem in enumerate(title_elements[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
            try:
                text = elem.text.strip()
                tag_name = elem.tag_name
                class_name = elem.get_attribute("class") or ""
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:50]}... (class: {class_name[:30]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # se- ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
        print("\nğŸ” se- ê´€ë ¨ í´ë˜ìŠ¤ ë¶„ì„:")
        se_elements = scraper.driver.find_elements("css selector", "[class*='se-']")
        print(f"   ë°œê²¬ëœ se- í´ë˜ìŠ¤: {len(se_elements)}ê°œ")
        
        for i, elem in enumerate(se_elements[:10]):
            try:
                text = elem.text.strip()
                class_name = elem.get_attribute("class") or ""
                tag_name = elem.tag_name
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:50]}... (class: {class_name[:50]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # content ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
        print("\nğŸ” content ê´€ë ¨ í´ë˜ìŠ¤ ë¶„ì„:")
        content_elements = scraper.driver.find_elements("css selector", "[class*='content'], [class*='Content']")
        print(f"   ë°œê²¬ëœ content í´ë˜ìŠ¤: {len(content_elements)}ê°œ")
        
        for i, elem in enumerate(content_elements[:10]):
            try:
                text = elem.text.strip()
                class_name = elem.get_attribute("class") or ""
                tag_name = elem.tag_name
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:50]}... (class: {class_name[:50]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # article ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
        print("\nğŸ” article ê´€ë ¨ í´ë˜ìŠ¤ ë¶„ì„:")
        article_elements = scraper.driver.find_elements("css selector", "[class*='article'], [class*='Article']")
        print(f"   ë°œê²¬ëœ article í´ë˜ìŠ¤: {len(article_elements)}ê°œ")
        
        for i, elem in enumerate(article_elements[:10]):
            try:
                text = elem.text.strip()
                class_name = elem.get_attribute("class") or ""
                tag_name = elem.tag_name
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:50]}... (class: {class_name[:50]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # ì‘ì„±ì ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ” ì‘ì„±ì ê´€ë ¨ ìš”ì†Œ ë¶„ì„:")
        author_elements = scraper.driver.find_elements("css selector", "[class*='nick'], [class*='author'], [class*='writer']")
        print(f"   ë°œê²¬ëœ ì‘ì„±ì ê´€ë ¨ ìš”ì†Œ: {len(author_elements)}ê°œ")
        
        for i, elem in enumerate(author_elements[:10]):
            try:
                text = elem.text.strip()
                class_name = elem.get_attribute("class") or ""
                tag_name = elem.tag_name
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:30]}... (class: {class_name[:30]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # ëŒ“ê¸€ ê´€ë ¨ ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ” ëŒ“ê¸€ ê´€ë ¨ ìš”ì†Œ ë¶„ì„:")
        comment_elements = scraper.driver.find_elements("css selector", "[class*='comment'], [class*='reply']")
        print(f"   ë°œê²¬ëœ ëŒ“ê¸€ ê´€ë ¨ ìš”ì†Œ: {len(comment_elements)}ê°œ")
        
        for i, elem in enumerate(comment_elements[:10]):
            try:
                text = elem.text.strip()
                class_name = elem.get_attribute("class") or ""
                tag_name = elem.tag_name
                if text:
                    print(f"   {i+1}. <{tag_name}> {text[:30]}... (class: {class_name[:30]})")
            except Exception as e:
                print(f"   {i+1}. ì˜¤ë¥˜: {e}")
        
        # ëª¨ë“  í´ë˜ìŠ¤ ì´ë¦„ ìˆ˜ì§‘
        print("\nğŸ” ëª¨ë“  í´ë˜ìŠ¤ ì´ë¦„ ìˆ˜ì§‘:")
        all_elements = scraper.driver.find_elements("css selector", "*")
        all_classes = set()
        
        for elem in all_elements:
            try:
                class_name = elem.get_attribute("class")
                if class_name:
                    all_classes.update(class_name.split())
            except:
                continue
        
        # ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í´ë˜ìŠ¤ë“¤ë§Œ í•„í„°ë§
        relevant_classes = []
        keywords = ['title', 'content', 'article', 'post', 'author', 'nick', 'comment', 'reply', 'se-']
        
        for class_name in all_classes:
            if any(keyword in class_name.lower() for keyword in keywords):
                relevant_classes.append(class_name)
        
        print(f"   ê´€ë ¨ í´ë˜ìŠ¤ {len(relevant_classes)}ê°œ ë°œê²¬:")
        for i, class_name in enumerate(sorted(relevant_classes)[:20]):  # ì²˜ìŒ 20ê°œë§Œ ì¶œë ¥
            print(f"   {i+1}. {class_name}")
        
        print("\nâœ… HTML êµ¬ì¡° ë¶„ì„ ì™„ë£Œ!")
        print("ğŸ“ ìƒì„±ëœ íŒŒì¼ë“¤:")
        print("   - debug_real_structure.png (ìŠ¤í¬ë¦°ìƒ·)")
        print("   - debug_page_source_full.html (ì „ì²´ HTML)")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    analyze_html_structure()
