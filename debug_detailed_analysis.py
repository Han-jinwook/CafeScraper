#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ ìƒì„¸ HTML êµ¬ì¡° ë¶„ì„
ì‹¤ì œ ê²Œì‹œê¸€ ì œëª©ê³¼ ë‚´ìš©ì˜ ìœ„ì¹˜ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
"""

import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json

def detailed_analysis():
    """ìƒì„¸ HTML êµ¬ì¡° ë¶„ì„"""
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # User-Agent ì„¤ì •
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        # WebDriver ì´ˆê¸°í™”
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("ğŸ” ë„¤ì´ë²„ ì¹´í˜ ìƒì„¸ HTML êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL
        test_url = "https://cafe.naver.com/f-e/cafes/27870803/articles/66575?boardtype=L&menuid=185&referrerAllArticles=false"
        
        print(f"ğŸ“„ ê²Œì‹œê¸€ ë¶„ì„: {test_url}")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(test_url)
        time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        driver.save_screenshot("debug_detailed_analysis.png")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ë¶„ì„
        page_source = driver.page_source
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(page_source, 'html.parser')
        
        print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ")
        
        # 1. ëª¨ë“  í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ìš”ì†Œë“¤ ì°¾ê¸°
        print("\nğŸ” ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ ë¶„ì„:")
        find_all_text_elements(driver, soup)
        
        # 2. ê²Œì‹œê¸€ ì œëª©ì´ ìˆì„ ìˆ˜ ìˆëŠ” ì˜ì—­ë“¤ ìƒì„¸ ë¶„ì„
        print("\nğŸ” ê²Œì‹œê¸€ ì œëª© ìƒì„¸ ë¶„ì„:")
        analyze_title_detailed(driver, soup)
        
        # 3. ê²Œì‹œê¸€ ë‚´ìš©ì´ ìˆì„ ìˆ˜ ìˆëŠ” ì˜ì—­ë“¤ ìƒì„¸ ë¶„ì„
        print("\nğŸ” ê²Œì‹œê¸€ ë‚´ìš© ìƒì„¸ ë¶„ì„:")
        analyze_content_detailed(driver, soup)
        
        # 4. í˜ì´ì§€ êµ¬ì¡° ì „ì²´ ë¶„ì„
        print("\nğŸ” í˜ì´ì§€ êµ¬ì¡° ì „ì²´ ë¶„ì„:")
        analyze_page_structure(driver, soup)
        
        driver.quit()
        print("\nâœ… ìƒì„¸ HTML êµ¬ì¡° ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")

def find_all_text_elements(driver, soup):
    """ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ ì°¾ê¸°"""
    
    # ëª¨ë“  div ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ê²ƒë“¤ ì°¾ê¸°
    all_divs = soup.find_all('div')
    text_elements = []
    
    for div in all_divs:
        text = div.get_text().strip()
        if text and len(text) > 10 and len(text) < 200:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
            classes = div.get('class', [])
            text_elements.append({
                'text': text,
                'classes': classes,
                'tag': div.name
            })
    
    print(f"   ğŸ“‹ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” div ìš”ì†Œ: {len(text_elements)}ê°œ")
    
    # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
    for i, elem in enumerate(text_elements[:10]):
        classes_str = ' '.join(elem['classes']) if elem['classes'] else 'no-class'
        print(f"     {i+1}. '{elem['text'][:50]}...' (í´ë˜ìŠ¤: {classes_str})")

def analyze_title_detailed(driver, soup):
    """ê²Œì‹œê¸€ ì œëª© ìƒì„¸ ë¶„ì„"""
    
    # ê°€ëŠ¥í•œ ì œëª© ì…€ë ‰í„°ë“¤
    title_selectors = [
        # ì¼ë°˜ì ì¸ ì œëª© ì…€ë ‰í„°
        'h1', 'h2', 'h3', 'h4', 'h5', 'h6',
        '.title', '.article-title', '.post-title',
        '.content-title', '.board-title',
        # ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™”
        '.cafe-title', '.article-title',
        # í´ë˜ìŠ¤ íŒ¨í„´
        '[class*="title"]', '[class*="Title"]',
        '[class*="article"]', '[class*="Article"]',
        '[class*="post"]', '[class*="Post"]',
        # ìµœì‹  ë„¤ì´ë²„ êµ¬ì¡°
        '.se-title', '.se-text',
        '[class*="se-"]'
    ]
    
    for selector in title_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:3]):
                    try:
                        text = elem.text.strip()
                        classes = elem.get_attribute('class') or ''
                        if text and len(text) > 5:
                            print(f"     {i+1}. '{text[:100]}...' (í´ë˜ìŠ¤: {classes})")
                    except:
                        pass
        except Exception as e:
            print(f"   âŒ {selector}: ì˜¤ë¥˜ - {e}")

def analyze_content_detailed(driver, soup):
    """ê²Œì‹œê¸€ ë‚´ìš© ìƒì„¸ ë¶„ì„"""
    
    # ê°€ëŠ¥í•œ ë‚´ìš© ì…€ë ‰í„°ë“¤
    content_selectors = [
        # ì¼ë°˜ì ì¸ ë‚´ìš© ì…€ë ‰í„°
        '.content', '.article-content', '.post-content',
        '.text', '.article-text', '.post-text',
        # ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™”
        '.cafe-content', '.article-content',
        # í´ë˜ìŠ¤ íŒ¨í„´
        '[class*="content"]', '[class*="Content"]',
        '[class*="text"]', '[class*="Text"]',
        '[class*="article"]', '[class*="Article"]',
        '[class*="post"]', '[class*="Post"]',
        # ìµœì‹  ë„¤ì´ë²„ êµ¬ì¡°
        '.se-main-container', '.se-component-content',
        '.se-text-paragraph', '.se-text',
        '[class*="se-"]'
    ]
    
    for selector in content_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:2]):
                    try:
                        text = elem.text.strip()
                        classes = elem.get_attribute('class') or ''
                        if text and len(text) > 20:
                            print(f"     {i+1}. '{text[:200]}...' (í´ë˜ìŠ¤: {classes})")
                    except:
                        pass
        except Exception as e:
            print(f"   âŒ {selector}: ì˜¤ë¥˜ - {e}")

def analyze_page_structure(driver, soup):
    """í˜ì´ì§€ êµ¬ì¡° ì „ì²´ ë¶„ì„"""
    
    # ì£¼ìš” ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
    main_containers = [
        'main', '.main', '#main',
        '.container', '.content', '.article',
        '.post', '.board', '.cafe',
        '[class*="Layout"]', '[class*="layout"]',
        '[class*="Content"]', '[class*="content"]'
    ]
    
    for selector in main_containers:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:2]):
                    try:
                        text = elem.text.strip()
                        classes = elem.get_attribute('class') or ''
                        if text and len(text) > 50:
                            print(f"     {i+1}. '{text[:100]}...' (í´ë˜ìŠ¤: {classes})")
                    except:
                        pass
        except Exception as e:
            print(f"   âŒ {selector}: ì˜¤ë¥˜ - {e}")

if __name__ == "__main__":
    detailed_analysis()
