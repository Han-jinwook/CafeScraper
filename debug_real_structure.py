#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ ì‹¤ì œ HTML êµ¬ì¡° ì™„ì „ ë¶„ì„
ì‹¤ì œ ê²Œì‹œê¸€ í˜ì´ì§€ì˜ ì •í™•í•œ êµ¬ì¡°ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.
"""

import os
import time
import json
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup

def analyze_real_structure():
    """ì‹¤ì œ ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ êµ¬ì¡° ì™„ì „ ë¶„ì„"""
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # User-Agent ì„¤ì •
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        # WebDriver ì´ˆê¸°í™”
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("ğŸ” ë„¤ì´ë²„ ì¹´í˜ ì‹¤ì œ HTML êµ¬ì¡° ì™„ì „ ë¶„ì„ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL
        test_url = "https://cafe.naver.com/f-e/cafes/27870803/articles/66575?boardtype=L&menuid=185&referrerAllArticles=false"
        
        print(f"ğŸ“„ ë¶„ì„ ëŒ€ìƒ: {test_url}")
        
        # í˜ì´ì§€ ë¡œë“œ
        driver.get(test_url)
        time.sleep(8)  # ì¶©ë¶„í•œ ë¡œë”© ëŒ€ê¸°
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        driver.save_screenshot("debug_real_structure.png")
        
        # í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥
        page_source = driver.page_source
        with open("debug_page_source_full.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        
        print("âœ… í˜ì´ì§€ ì†ŒìŠ¤ ì €ì¥ ì™„ë£Œ: debug_page_source_full.html")
        
        # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
        soup = BeautifulSoup(page_source, 'html.parser')
        
        # 1. ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ ì™„ì „ ë¶„ì„
        print("\nğŸ” ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ ì™„ì „ ë¶„ì„:")
        analyze_all_text_elements(driver, soup)
        
        # 2. ì‹¤ì œ ê²Œì‹œê¸€ ì œëª© ì°¾ê¸°
        print("\nğŸ” ì‹¤ì œ ê²Œì‹œê¸€ ì œëª© ì°¾ê¸°:")
        find_real_title(driver, soup)
        
        # 3. ì‹¤ì œ ê²Œì‹œê¸€ ë‚´ìš© ì°¾ê¸°
        print("\nğŸ” ì‹¤ì œ ê²Œì‹œê¸€ ë‚´ìš© ì°¾ê¸°:")
        find_real_content(driver, soup)
        
        # 4. ì‹¤ì œ ì‘ì„±ì ì°¾ê¸°
        print("\nğŸ” ì‹¤ì œ ì‘ì„±ì ì°¾ê¸°:")
        find_real_author(driver, soup)
        
        # 5. í˜ì´ì§€ êµ¬ì¡° ì™„ì „ ë¶„ì„
        print("\nğŸ” í˜ì´ì§€ êµ¬ì¡° ì™„ì „ ë¶„ì„:")
        analyze_complete_structure(driver, soup)
        
        driver.quit()
        print("\nâœ… ì™„ì „ ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")

def analyze_all_text_elements(driver, soup):
    """ëª¨ë“  í…ìŠ¤íŠ¸ ìš”ì†Œ ì™„ì „ ë¶„ì„"""
    
    # ëª¨ë“  ìš”ì†Œì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
    all_elements = soup.find_all(['div', 'span', 'p', 'h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'article', 'section'])
    
    text_elements = []
    for elem in all_elements:
        text = elem.get_text().strip()
        if text and len(text) > 5 and len(text) < 500:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
            classes = elem.get('class', [])
            text_elements.append({
                'text': text,
                'classes': classes,
                'tag': elem.name,
                'id': elem.get('id', '')
            })
    
    print(f"   ğŸ“‹ í…ìŠ¤íŠ¸ê°€ ìˆëŠ” ìš”ì†Œ: {len(text_elements)}ê°œ")
    
    # ì²˜ìŒ 20ê°œ ì¶œë ¥
    for i, elem in enumerate(text_elements[:20]):
        classes_str = ' '.join(elem['classes']) if elem['classes'] else 'no-class'
        id_str = elem['id'] if elem['id'] else 'no-id'
        print(f"     {i+1}. [{elem['tag']}] '{elem['text'][:100]}...' (í´ë˜ìŠ¤: {classes_str}, ID: {id_str})")

def find_real_title(driver, soup):
    """ì‹¤ì œ ê²Œì‹œê¸€ ì œëª© ì°¾ê¸°"""
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ì œëª© ì…€ë ‰í„° ì‹œë„
    title_candidates = []
    
    # 1. ëª¨ë“  h íƒœê·¸ í™•ì¸
    h_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    for h in h_tags:
        text = h.get_text().strip()
        if text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in text:  # ì¹´í˜ ì œëª© ì œì™¸
            title_candidates.append({
                'text': text,
                'tag': h.name,
                'classes': h.get('class', []),
                'selector': h.name
            })
    
    # 2. title ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    title_elements = soup.find_all(attrs={'class': lambda x: x and any('title' in cls.lower() for cls in x)})
    for elem in title_elements:
        text = elem.get_text().strip()
        if text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in text:
            title_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    # 3. se- ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    se_elements = soup.find_all(attrs={'class': lambda x: x and any('se-' in cls for cls in x)})
    for elem in se_elements:
        text = elem.get_text().strip()
        if text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in text:
            title_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    print(f"   ğŸ“‹ ì œëª© í›„ë³´: {len(title_candidates)}ê°œ")
    for i, candidate in enumerate(title_candidates[:10]):
        classes_str = ' '.join(candidate['classes']) if candidate['classes'] else 'no-class'
        print(f"     {i+1}. '{candidate['text'][:100]}...' (íƒœê·¸: {candidate['tag']}, í´ë˜ìŠ¤: {classes_str})")

def find_real_content(driver, soup):
    """ì‹¤ì œ ê²Œì‹œê¸€ ë‚´ìš© ì°¾ê¸°"""
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ë‚´ìš© ì…€ë ‰í„° ì‹œë„
    content_candidates = []
    
    # 1. se- ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    se_elements = soup.find_all(attrs={'class': lambda x: x and any('se-' in cls for cls in x)})
    for elem in se_elements:
        text = elem.get_text().strip()
        if text and len(text) > 20:  # ì˜ë¯¸ìˆëŠ” ë‚´ìš©ë§Œ
            content_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    # 2. content ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    content_elements = soup.find_all(attrs={'class': lambda x: x and any('content' in cls.lower() for cls in x)})
    for elem in content_elements:
        text = elem.get_text().strip()
        if text and len(text) > 20:
            content_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    # 3. article ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    article_elements = soup.find_all(attrs={'class': lambda x: x and any('article' in cls.lower() for cls in x)})
    for elem in article_elements:
        text = elem.get_text().strip()
        if text and len(text) > 20:
            content_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    print(f"   ğŸ“‹ ë‚´ìš© í›„ë³´: {len(content_candidates)}ê°œ")
    for i, candidate in enumerate(content_candidates[:10]):
        classes_str = ' '.join(candidate['classes']) if candidate['classes'] else 'no-class'
        print(f"     {i+1}. '{candidate['text'][:200]}...' (íƒœê·¸: {candidate['tag']}, í´ë˜ìŠ¤: {classes_str})")

def find_real_author(driver, soup):
    """ì‹¤ì œ ì‘ì„±ì ì°¾ê¸°"""
    
    # ê°€ëŠ¥í•œ ëª¨ë“  ì‘ì„±ì ì…€ë ‰í„° ì‹œë„
    author_candidates = []
    
    # 1. nick ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    nick_elements = soup.find_all(attrs={'class': lambda x: x and any('nick' in cls.lower() for cls in x)})
    for elem in nick_elements:
        text = elem.get_text().strip()
        if text and len(text) < 50:  # ì‘ì„±ìëª…ì€ ë³´í†µ ì§§ìŒ
            author_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    # 2. author ê´€ë ¨ í´ë˜ìŠ¤ í™•ì¸
    author_elements = soup.find_all(attrs={'class': lambda x: x and any('author' in cls.lower() for cls in x)})
    for elem in author_elements:
        text = elem.get_text().strip()
        if text and len(text) < 50:
            author_candidates.append({
                'text': text,
                'tag': elem.name,
                'classes': elem.get('class', []),
                'selector': f".{'.'.join(elem.get('class', []))}"
            })
    
    print(f"   ğŸ“‹ ì‘ì„±ì í›„ë³´: {len(author_candidates)}ê°œ")
    for i, candidate in enumerate(author_candidates[:10]):
        classes_str = ' '.join(candidate['classes']) if candidate['classes'] else 'no-class'
        print(f"     {i+1}. '{candidate['text']}' (íƒœê·¸: {candidate['tag']}, í´ë˜ìŠ¤: {classes_str})")

def analyze_complete_structure(driver, soup):
    """í˜ì´ì§€ êµ¬ì¡° ì™„ì „ ë¶„ì„"""
    
    # ì£¼ìš” ì»¨í…Œì´ë„ˆë“¤ ì°¾ê¸°
    main_containers = [
        'main', '.main', '#main',
        '.container', '.content', '.article',
        '.post', '.board', '.cafe',
        '[class*="Layout"]', '[class*="layout"]',
        '[class*="Content"]', '[class*="content"]',
        '[class*="Article"]', '[class*="article"]'
    ]
    
    for selector in main_containers:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:2]):
                    try:
                        text = elem.text.strip()
                        classes = elem.get_attribute('class') or ''
                        if text and len(text) > 50:
                            print(f"     {i+1}. '{text[:100]}...' (í´ë˜ìŠ¤: {classes})")
                    except:
                        pass
        except Exception as e:
            print(f"   âŒ {selector}: ì˜¤ë¥˜ - {e}")

if __name__ == "__main__":
    analyze_real_structure()
