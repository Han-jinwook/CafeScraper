#!/usr/bin/env python3
"""
ìµœì¢… ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ - ì—…ë°ì´íŠ¸ëœ ì…€ë ‰í„°ë¡œ ì‹¤ì œ ìŠ¤í¬ë˜í•‘
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def test_final_scraping():
    """ìµœì¢… ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # ì¤‘ê³ ë‚˜ë¼ ì¹´í˜ ì ‘ì†
        print("ğŸŒ ì¤‘ê³ ë‚˜ë¼ ì¹´í˜ ì ‘ì†...")
        scraper.driver.get("https://cafe.naver.com/joonggonara")
        time.sleep(5)
        
        # JavaScript ë¡œë”© ëŒ€ê¸°
        print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (60ì´ˆ)")
        time.sleep(60)
        
        # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
        print("ğŸ” ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°...")
        article_selectors = [
            "a[href*='ArticleRead']",
            "a[href*='ArticleRead.nhn']",
            ".article a",
            ".board_list a",
            "tr td a",
            "a[href*='cafe.naver.com']"
        ]
        
        article_links = []
        for selector in article_selectors:
            try:
                links = scraper.driver.find_elements("css selector", selector)
                if links:
                    article_links = links
                    print(f"   {selector}: {len(links)}ê°œ ë°œê²¬")
                    break
            except:
                continue
        
        if article_links:
            print(f"âœ… ê²Œì‹œê¸€ ë§í¬ {len(article_links)}ê°œ ë°œê²¬!")
            
            # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë§í¬ í´ë¦­
            first_link = article_links[0]
            article_url = first_link.get_attribute("href")
            print(f"ğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ì ‘ì†: {article_url}")
            
            # ê²Œì‹œê¸€ í˜ì´ì§€ë¡œ ì´ë™
            scraper.driver.get(article_url)
            time.sleep(5)
            
            # JavaScript ë¡œë”© ëŒ€ê¸°
            print("â³ ê²Œì‹œê¸€ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (60ì´ˆ)")
            time.sleep(60)
            
            # ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸:")
            
            # ì œëª© ì¶”ì¶œ
            print("ğŸ” ì œëª© ì¶”ì¶œ:")
            title_selectors = [
                "h3.title",
                "h2.title", 
                "h1.title",
                ".title"
            ]
            
            title = "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            for selector in title_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text and len(text) > 5:
                                title = text
                                print(f"   âœ… {selector}: {title}")
                                break
                        if title != "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                            break
                except:
                    continue
            
            if title == "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                print("   âŒ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë‚´ìš© ì¶”ì¶œ
            print("\nğŸ” ë‚´ìš© ì¶”ì¶œ:")
            content_selectors = [
                ".content",
                ".se-main-container",
                ".se-component-content",
                ".se-text-paragraph"
            ]
            
            content = "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            for selector in content_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text and len(text) > 10:
                                content = text[:100] + "..." if len(text) > 100 else text
                                print(f"   âœ… {selector}: {content}")
                                break
                        if content != "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                            break
                except:
                    continue
            
            if content == "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                print("   âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ì‘ì„±ì ì¶”ì¶œ
            print("\nğŸ” ì‘ì„±ì ì¶”ì¶œ:")
            author_selectors = [
                ".nick",
                ".nickname",
                ".author", 
                ".writer",
                "[class*='nick']",
                "[class*='author']",
                "[class*='writer']"
            ]
            
            author = "ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ"
            for selector in author_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text:
                                author = text
                                print(f"   âœ… {selector}: {author}")
                                break
                        if author != "ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                            break
                except:
                    continue
            
            if author == "ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                print("   âŒ ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ëŒ“ê¸€ ì¶”ì¶œ
            print("\nğŸ” ëŒ“ê¸€ ì¶”ì¶œ:")
            comment_selectors = [
                ".comment_area",
                ".LinkComment",
                ".comment",
                ".reply"
            ]
            
            comments = []
            for selector in comment_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                        comments = elements
                        break
                except:
                    continue
            
            if not comments:
                print("   âŒ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ê²°ê³¼ ìš”ì•½
            print("\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼ ìš”ì•½:")
            print(f"   ì œëª©: {title}")
            print(f"   ë‚´ìš©: {content[:50]}..." if len(content) > 50 else f"   ë‚´ìš©: {content}")
            print(f"   ì‘ì„±ì: {author}")
            print(f"   ëŒ“ê¸€: {len(comments)}ê°œ")
            
            # ì„±ê³µë¥  ê³„ì‚°
            success_count = 0
            if title != "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                success_count += 1
            if content != "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                success_count += 1
            if author != "ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                success_count += 1
            if comments:
                success_count += 1
            
            success_rate = (success_count / 4) * 100
            print(f"\nğŸ¯ ì„±ê³µë¥ : {success_rate:.1f}% ({success_count}/4)")
            
            if success_rate >= 75:
                print("ğŸ‰ ìŠ¤í¬ë˜í•‘ ì„±ê³µ! ë°°ì¹˜ í…ŒìŠ¤íŠ¸ë¥¼ ì§„í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            else:
                print("âš ï¸ ìŠ¤í¬ë˜í•‘ ë¶€ë¶„ ì„±ê³µ. ì¶”ê°€ ê°œì„ ì´ í•„ìš”í•©ë‹ˆë‹¤.")
            
        else:
            print("âŒ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    test_final_scraping()
