#!/usr/bin/env python3
"""
ë°ìŠ¤í¬í†± ë²„ì „ìœ¼ë¡œ ê°•ì œ ì ‘ê·¼í•˜ì—¬ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def test_desktop_version():
    """ë°ìŠ¤í¬í†± ë²„ì „ìœ¼ë¡œ ê°•ì œ ì ‘ê·¼í•˜ì—¬ í…ŒìŠ¤íŠ¸"""
    
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # ë°ìŠ¤í¬í†± ë²„ì „ìœ¼ë¡œ ê°•ì œ ì ‘ê·¼
        desktop_url = "https://cafe.naver.com/joonggonara"
        print(f"ğŸŒ ë°ìŠ¤í¬í†± ë²„ì „ ì ‘ì†: {desktop_url}")
        
        # User-Agentë¥¼ ë°ìŠ¤í¬í†±ìœ¼ë¡œ ì„¤ì •
        scraper.driver.execute_script("Object.defineProperty(navigator, 'userAgent', {get: function () {return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36';}})")
        
        scraper.driver.get(desktop_url)
        time.sleep(5)  # ê¸°ë³¸ ë¡œë”© ëŒ€ê¸°
        
        # JavaScript ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
        print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (60ì´ˆ)")
        time.sleep(60)
        
        # í˜„ì¬ URL í™•ì¸
        current_url = scraper.driver.current_url
        print(f"ğŸ“ í˜„ì¬ URL: {current_url}")
        
        # í˜ì´ì§€ ì œëª© í™•ì¸
        page_title = scraper.driver.title
        print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
        
        # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
        scraper.driver.save_screenshot("debug_desktop_version.png")
        print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_desktop_version.png")
        
        # HTML ì €ì¥
        page_source = scraper.driver.page_source
        with open("debug_desktop_version.html", "w", encoding="utf-8") as f:
            f.write(page_source)
        print("ğŸ’¾ HTML ì €ì¥: debug_desktop_version.html")
        
        # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
        print("\nğŸ” ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°...")
        article_selectors = [
            "a[href*='ArticleRead']",
            "a[href*='ArticleRead.nhn']",
            ".article a",
            ".board_list a",
            "tr td a",
            "a[href*='cafe.naver.com']"
        ]
        
        article_links = []
        for selector in article_selectors:
            try:
                links = scraper.driver.find_elements("css selector", selector)
                if links:
                    article_links = links
                    print(f"   {selector}: {len(links)}ê°œ ë°œê²¬")
                    break
            except:
                continue
        
        if article_links:
            print(f"âœ… ê²Œì‹œê¸€ ë§í¬ {len(article_links)}ê°œ ë°œê²¬!")
            
            # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë§í¬ í´ë¦­
            first_link = article_links[0]
            article_url = first_link.get_attribute("href")
            print(f"ğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ì ‘ì†: {article_url}")
            
            # ê²Œì‹œê¸€ í˜ì´ì§€ë¡œ ì´ë™
            scraper.driver.get(article_url)
            time.sleep(5)  # ê¸°ë³¸ ë¡œë”© ëŒ€ê¸°
            
            # JavaScript ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
            print("â³ ê²Œì‹œê¸€ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (60ì´ˆ)")
            time.sleep(60)
            
            # í˜„ì¬ URL í™•ì¸
            current_url = scraper.driver.current_url
            print(f"ğŸ“ í˜„ì¬ URL: {current_url}")
            
            # í˜ì´ì§€ ì œëª© í™•ì¸
            page_title = scraper.driver.title
            print(f"ğŸ“„ í˜ì´ì§€ ì œëª©: {page_title}")
            
            # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
            scraper.driver.save_screenshot("debug_article_desktop.png")
            print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_article_desktop.png")
            
            # HTML ì €ì¥
            page_source = scraper.driver.page_source
            with open("debug_article_desktop.html", "w", encoding="utf-8") as f:
                f.write(page_source)
            print("ğŸ’¾ HTML ì €ì¥: debug_article_desktop.html")
            
            # ì—…ë°ì´íŠ¸ëœ ìŠ¤í¬ë˜í•‘ ë¡œì§ í…ŒìŠ¤íŠ¸
            print("\nğŸ§ª ì—…ë°ì´íŠ¸ëœ ìŠ¤í¬ë˜í•‘ ë¡œì§ í…ŒìŠ¤íŠ¸:")
            
            # ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print("ğŸ” ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
            title_selectors = [
                "h3.title",
                "h2.title", 
                "h1.title",
                ".title",
                ".se-title-text",
                ".se-fs-",
                ".se-component-content h1",
                ".se-component-content h2", 
                ".se-component-content h3",
                ".se-text-paragraph"
            ]
            
            title_found = False
            for selector in title_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text and len(text) > 5:
                                print(f"   âœ… {selector}: {text[:50]}...")
                                title_found = True
                                break
                        if title_found:
                            break
                except:
                    continue
            
            if not title_found:
                print("   âŒ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print("\nğŸ” ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
            content_selectors = [
                ".content",
                ".se-main-container",
                ".se-component-content",
                ".se-text-paragraph",
                ".se-text",
                ".se-component",
                ".article_content",
                ".post_content"
            ]
            
            content_found = False
            for selector in content_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text and len(text) > 10:
                                print(f"   âœ… {selector}: {text[:50]}...")
                                content_found = True
                                break
                        if content_found:
                            break
                except:
                    continue
            
            if not content_found:
                print("   âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ì‘ì„±ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print("\nğŸ” ì‘ì„±ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
            author_selectors = [
                ".nick",
                ".nickname",
                ".author", 
                ".writer",
                "[class*='nick']",
                "[class*='author']",
                "[class*='writer']"
            ]
            
            author_found = False
            for selector in author_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        for elem in elements:
                            text = elem.text.strip()
                            if text:
                                print(f"   âœ… {selector}: {text[:30]}...")
                                author_found = True
                                break
                        if author_found:
                            break
                except:
                    continue
            
            if not author_found:
                print("   âŒ ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
            print("\nğŸ” ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
            comment_selectors = [
                ".comment_area",
                ".LinkComment",
                ".comment",
                ".reply",
                "[class*='comment']",
                "[class*='reply']"
            ]
            
            comment_found = False
            for selector in comment_selectors:
                try:
                    elements = scraper.driver.find_elements("css selector", selector)
                    if elements:
                        print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                        comment_found = True
                        break
                except:
                    continue
            
            if not comment_found:
                print("   âŒ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
        else:
            print("âŒ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    test_desktop_version()
