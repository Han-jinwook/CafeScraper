#!/usr/bin/env python3
"""
ì§ì ‘ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
ì›¹ UI ì—†ì´ ì§ì ‘ ë°°ì¹˜ í¬ë¡¤ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.scraper.naver import NaverScraper

def test_direct_scraping():
    """ì§ì ‘ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª ì§ì ‘ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘")
    print("=" * 50)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # 1. ë¸Œë¼ìš°ì € ì‹œì‘
        print("1ï¸âƒ£ ë¸Œë¼ìš°ì € ì‹œì‘")
        scraper.start_browser()
        print("âœ… ë¸Œë¼ìš°ì € ì‹œì‘ ì„±ê³µ")
        
        # 2. ì¿ í‚¤ ë¡œë“œ
        print("\n2ï¸âƒ£ ì¿ í‚¤ ë¡œë“œ")
        scraper._load_cookies()
        print("âœ… ì¿ í‚¤ ë¡œë“œ ì™„ë£Œ")
        
        # 3. ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        print("\n3ï¸âƒ£ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸")
        login_status = scraper._check_login_status()
        if login_status:
            print("âœ… ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì„±ê³µ")
        else:
            print("âŒ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨")
            return
        
        # 4. ì¹´í˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
        print("\n4ï¸âƒ£ ì¹´í˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ")
        cafe_url = "https://cafe.naver.com/sundreamd"
        boards = scraper.get_cafe_boards(cafe_url)
        print(f"âœ… ê²Œì‹œíŒ {len(boards)}ê°œ ì¡°íšŒ ì„±ê³µ")
        
        # 5. íŠ¹ì • ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ (â˜…ê°€ì…ì¸ì‚¬)
        print("\n5ï¸âƒ£ íŠ¹ì • ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘")
        target_board = None
        for board in boards:
            if board['menu_name'] == 'â˜…ê°€ì…ì¸ì‚¬':
                target_board = board
                break
        
        if not target_board:
            print("âŒ â˜…ê°€ì…ì¸ì‚¬ ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
            return
        
        print(f"âœ… ëŒ€ìƒ ê²Œì‹œíŒ: {target_board['menu_name']} (ID: {target_board['menu_id']})")
        
        # 6. ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ (ìµœëŒ€ 2í˜ì´ì§€)
        print("\n6ï¸âƒ£ ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘")
        board_results = scraper.scrape_board_articles(target_board['board_url'], max_pages=2)
        print(f"âœ… ê²Œì‹œíŒì—ì„œ {len(board_results)}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        
        if not board_results:
            print("âŒ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
            return
        
        # 7. í‚¤ì›Œë“œ í•„í„°ë§ í…ŒìŠ¤íŠ¸
        print("\n7ï¸âƒ£ í‚¤ì›Œë“œ í•„í„°ë§ í…ŒìŠ¤íŠ¸")
        search_keywords = ["ê±´ì„ "]
        filtered_articles = []
        
        for article in board_results:
            title = article.get('title', '').lower()
            if any(keyword.lower() in title for keyword in search_keywords):
                filtered_articles.append(article)
                print(f"âœ… í‚¤ì›Œë“œ ì¼ì¹˜: {article.get('title', 'N/A')[:50]}...")
        
        print(f"âœ… í‚¤ì›Œë“œ í•„í„°ë§ ê²°ê³¼: {len(filtered_articles)}ê°œ ê²Œì‹œê¸€")
        
        if not filtered_articles:
            print("âš ï¸ 'ê±´ì„ ' í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê²Œì‹œê¸€ì´ ì—†ìŠµë‹ˆë‹¤")
            print("ğŸ” ì „ì²´ ê²Œì‹œê¸€ ì œëª© í™•ì¸:")
            for i, article in enumerate(board_results[:5], 1):
                print(f"   {i}. {article.get('title', 'N/A')}")
            return
        
        # 8. ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ (ìµœëŒ€ 2ê°œ)
        print("\n8ï¸âƒ£ ìƒì„¸ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸")
        test_articles = filtered_articles[:2]  # ìµœëŒ€ 2ê°œë§Œ í…ŒìŠ¤íŠ¸
        
        for i, article in enumerate(test_articles, 1):
            print(f"\nğŸ“„ ê²Œì‹œê¸€ {i}/{len(test_articles)} ìŠ¤í¬ë˜í•‘")
            print(f"   ì œëª©: {article.get('title', 'N/A')}")
            print(f"   URL: {article.get('article_url', 'N/A')}")
            
            try:
                # ìƒì„¸ ìŠ¤í¬ë˜í•‘
                detailed_result = scraper.scrape_article(
                    article['article_url'],
                    include_nicks=None,
                    exclude_nicks=None
                )
                
                print(f"   âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ")
                print(f"   ğŸ“ ë‚´ìš© ê¸¸ì´: {len(detailed_result.get('content_text', ''))}")
                print(f"   ğŸ’¬ ëŒ“ê¸€ ìˆ˜: {len(detailed_result.get('comments', []))}")
                print(f"   ğŸ–¼ï¸ ì´ë¯¸ì§€ ìˆ˜: {len(detailed_result.get('images_base64', []))}")
                
            except Exception as e:
                print(f"   âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {e}")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ì§ì ‘ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        if scraper.driver:
            scraper.close()
            print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œë¨")

if __name__ == "__main__":
    test_direct_scraping()

