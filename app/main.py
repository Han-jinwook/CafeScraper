import os
import logging
from fastapi import FastAPI, Body
from fastapi.responses import JSONResponse, FileResponse
from fastapi.staticfiles import StaticFiles
from pydantic import BaseModel

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('server.log', encoding='utf-8'),
        logging.StreamHandler()
    ]
)

# Lazy imports for optional heavy deps
try:
	from app.scraper.naver import NaverScraper
except Exception:
	NaverScraper = None  # type: ignore

from app.utils.csv_writer import append_article_bundle_row

app = FastAPI(title="CafeScraper", version="0.1.0")

SESSIONS_DIR = os.path.abspath(os.path.join(os.getcwd(), "sessions"))
OUTPUTS_DIR = os.path.abspath(os.path.join(os.getcwd(), "outputs"))
SNAPSHOTS_DIR = os.path.abspath(os.path.join(os.getcwd(), "snapshots"))
STATIC_DIR = os.path.abspath(os.path.join(os.getcwd(), "app", "static"))

for _d in (SESSIONS_DIR, OUTPUTS_DIR, SNAPSHOTS_DIR, STATIC_DIR):
	os.makedirs(_d, exist_ok=True)

# ì •ì  íŒŒì¼ ì„œë¹™
app.mount("/static", StaticFiles(directory=STATIC_DIR), name="static")


class CommentFilter(BaseModel):
	include: list[str] | None = None
	exclude: list[str] | None = None


class ScrapeArticlePayload(BaseModel):
	url: str
	cafe_id: str | None = None
	comment_filter: CommentFilter | None = None


class ScrapeBoardPayload(BaseModel):
	board_url: str
	max_pages: int = 5
	comment_filter: CommentFilter | None = None


class ScrapeMultipleArticlesPayload(BaseModel):
	article_urls: list[str]
	comment_filter: CommentFilter | None = None


class CafeBoardsPayload(BaseModel):
	cafe_url: str
	cafe_name: str | None = None


class CafeScrapingPayload(BaseModel):
	cafe_url: str
	max_pages: int = 5
	all_boards: bool = True
	selected_boards: list[str] = []
	comment_filter: CommentFilter | None = None


class BatchScrapingPayload(BaseModel):
	cafe_url: str
	max_pages: int = 5
	all_boards: bool = True
	selected_boards: list[str] = []
	search_keywords: list[str] = []
	post_authors: list[str] = []
	comment_authors: list[str] = []
	max_articles: int = 1000
	image_processing: str = "base64"  # none, base64, server
	period: str = "all"  # all, 1month, 6months, 1year, custom
	delay_between_requests: int = 3


@app.get("/")
async def root():
	"""ì›¹ UI í™ˆí˜ì´ì§€"""
	return FileResponse(os.path.join(STATIC_DIR, "index.html"))


@app.get("/.well-known/appspecific/com.chrome.devtools.json")
async def chrome_devtools():
	"""Chrome DevTools ë©”íƒ€ë°ì´í„°"""
	return {"version": "1.0", "name": "CafeScraper API"}


@app.get("/favicon.ico")
async def favicon():
	"""íŒŒë¹„ì½˜ ìš”ì²­ ì²˜ë¦¬"""
	from fastapi.responses import Response
	return Response(status_code=204)  # No Content

@app.get("/health")
async def health() -> dict:
	return {"ok": True}


@app.post("/login/start")
async def login_start() -> JSONResponse:
	"""Start manual login process with browser window."""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		success = await scraper.ensure_logged_in()
		await scraper.close()
		
		if success:
			return JSONResponse({
				"status": "success",
				"message": "Login completed successfully. Cookies saved.",
				"sessions_dir": str(SESSIONS_DIR),
			})
		else:
			return JSONResponse({
				"status": "failed", 
				"message": "Login failed. Please try again.",
				"sessions_dir": str(SESSIONS_DIR),
			})
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"Login error: {str(e)}",
			"sessions_dir": str(SESSIONS_DIR),
		}, status_code=500)


@app.post("/scrape/article")
async def scrape_single_article(payload: ScrapeArticlePayload) -> JSONResponse:
	"""Scrape a single article with comments and images."""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		# Extract comment filters
		include_nicks = payload.comment_filter.include if payload.comment_filter else None
		exclude_nicks = payload.comment_filter.exclude if payload.comment_filter else None
		
		# Perform actual scraping
		result = await scraper.scrape_article(
			payload.url, 
			include_nicks, 
			exclude_nicks
		)

		csv_path = append_article_bundle_row(OUTPUTS_DIR, result)
		await scraper.close()
		
		return JSONResponse({
			"status": "success",
			"saved_csv": csv_path,
			"message": "Article scraped and saved to CSV",
			"result": result
		})
		
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"Scraping failed: {str(e)}",
		}, status_code=500)


@app.post("/scrape/board")
async def scrape_board_articles(payload: ScrapeBoardPayload) -> JSONResponse:
	"""Scrape articles from a board page with pagination."""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		# Extract comment filters
		include_nicks = payload.comment_filter.include if payload.comment_filter else None
		exclude_nicks = payload.comment_filter.exclude if payload.comment_filter else None
		
		print(f"ğŸ“Š ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {payload.board_url}")
		print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {payload.max_pages}")
		
		# Get article list from board
		articles = await scraper.scrape_board_articles(payload.board_url, payload.max_pages)
		
		if not articles:
			await scraper.close()
			return JSONResponse({
				"status": "warning",
				"message": "No articles found on the board",
				"articles_found": 0,
				"articles_scraped": 0,
				"saved_csvs": [],
				"results": []
			})
		
		# Extract URLs for detailed scraping
		article_urls = [article["article_url"] for article in articles]
		
		print(f"ğŸ“Š ë°œê²¬ëœ ê²Œì‹œê¸€: {len(article_urls)}ê°œ")
		
		# Scrape detailed information for each article
		detailed_results = await scraper.scrape_multiple_articles(article_urls, include_nicks, exclude_nicks)
		
		# Save to CSV
		csv_paths = []
		successful_results = []
		
		for result in detailed_results:
			if "error" not in result:
				csv_path = append_article_bundle_row(OUTPUTS_DIR, result)
				csv_paths.append(csv_path)
				successful_results.append(result)
		
		await scraper.close()
		
		success_count = len(successful_results)
		error_count = len(detailed_results) - success_count
		
		return JSONResponse({
			"status": "success",
			"message": f"Board scraped: {success_count} articles processed successfully, {error_count} errors",
			"articles_found": len(articles),
			"articles_scraped": success_count,
			"articles_failed": error_count,
			"saved_csvs": csv_paths,
			"results": detailed_results
		})
		
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"Board scraping failed: {str(e)}",
		}, status_code=500)


@app.post("/scrape/multiple")
async def scrape_multiple_articles(payload: ScrapeMultipleArticlesPayload) -> JSONResponse:
	"""Scrape multiple articles from a list of URLs."""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		# Extract comment filters
		include_nicks = payload.comment_filter.include if payload.comment_filter else None
		exclude_nicks = payload.comment_filter.exclude if payload.comment_filter else None
		
		# Scrape multiple articles
		results = await scraper.scrape_multiple_articles(payload.article_urls, include_nicks, exclude_nicks)
		
		# Save to CSV
		csv_paths = []
		for result in results:
			csv_path = append_article_bundle_row(OUTPUTS_DIR, result)
			csv_paths.append(csv_path)
		
		await scraper.close()
		
		return JSONResponse({
			"status": "success",
			"message": f"Multiple articles scraped: {len(results)} articles processed",
			"saved_csvs": csv_paths,
			"results": results
		})
		
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"Multiple articles scraping failed: {str(e)}",
		}, status_code=500)


@app.post("/cafe/boards")
async def get_cafe_boards(payload: CafeBoardsPayload) -> JSONResponse:
	"""ì¹´í˜ì˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ"""
	try:
		print(f"ğŸ”„ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì‹œì‘: {payload.cafe_url}")
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		# ì¹´í˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
		boards = await scraper.get_cafe_boards(payload.cafe_url)
		
		await scraper.close()
		
		if not boards:
			return JSONResponse({
				"status": "warning",
				"message": "ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì¹´í˜ URLì„ í™•ì¸í•˜ê±°ë‚˜ ë¡œê·¸ì¸ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”.",
				"boards": []
			})
		
		print(f"âœ… ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì„±ê³µ: {len(boards)}ê°œ")
		return JSONResponse({
			"status": "success",
			"message": f"ê²Œì‹œíŒ ëª©ë¡ì„ ì¡°íšŒí–ˆìŠµë‹ˆë‹¤. ({len(boards)}ê°œ)",
			"boards": boards
		})
		
	except Exception as e:
		error_msg = str(e)
		print(f"âŒ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {error_msg}")
		
		if "Login required" in error_msg:
			error_msg = "ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € ë¡œê·¸ì¸ì„ ì§„í–‰í•˜ì„¸ìš”."
		elif "NoneType" in error_msg or "Browser" in error_msg:
			error_msg = "ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì˜¤ë¥˜ì…ë‹ˆë‹¤. ì„œë²„ë¥¼ ì¬ì‹œì‘í•˜ì„¸ìš”."
		elif "timeout" in error_msg.lower():
			error_msg = "í˜ì´ì§€ ë¡œë”© ì‹œê°„ ì´ˆê³¼ì…ë‹ˆë‹¤. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•˜ì„¸ìš”."
		
		return JSONResponse({
			"status": "error",
			"message": f"ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {error_msg}",
		}, status_code=500)


@app.post("/scrape/cafe")
async def scrape_cafe(payload: CafeScrapingPayload) -> JSONResponse:
	"""ì¹´í˜ ì „ì²´ ë˜ëŠ” íŠ¹ì • ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘"""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		# Extract comment filters
		include_nicks = payload.comment_filter.include if payload.comment_filter else None
		exclude_nicks = payload.comment_filter.exclude if payload.comment_filter else None
		
		print(f"ğŸ“Š ì¹´í˜ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {payload.cafe_url}")
		print(f"ğŸ“„ ì „ì²´ ê²Œì‹œíŒ: {payload.all_boards}")
		if not payload.all_boards:
			print(f"ğŸ“„ ì„ íƒëœ ê²Œì‹œíŒ: {payload.selected_boards}")
		
		# ì¹´í˜ ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
		results = await scraper.scrape_cafe(
			payload.cafe_url,
			payload.max_pages,
			payload.all_boards,
			payload.selected_boards,
			include_nicks,
			exclude_nicks
		)
		
		# Save to CSV
		csv_paths = []
		successful_results = []
		
		for result in results:
			if "error" not in result:
				csv_path = append_article_bundle_row(OUTPUTS_DIR, result)
				csv_paths.append(csv_path)
				successful_results.append(result)
		
		await scraper.close()
		
		success_count = len(successful_results)
		error_count = len(results) - success_count
		
		return JSONResponse({
			"status": "success",
			"message": f"ì¹´í˜ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {success_count}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨",
			"articles_scraped": success_count,
			"articles_failed": error_count,
			"saved_csvs": csv_paths,
			"results": results
		})
		
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"ì¹´í˜ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨: {str(e)}",
		}, status_code=500)


@app.post("/scrape/batch")
async def batch_scraping(payload: BatchScrapingPayload) -> JSONResponse:
	"""ë°°ì¹˜ í¬ë¡¤ë§ - í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ì‘ì„±ì í•„í„°ë§ í¬í•¨"""
	try:
		scraper = NaverScraper(SESSIONS_DIR, SNAPSHOTS_DIR)
		
		print(f"ğŸ”„ ë°°ì¹˜ í¬ë¡¤ë§ ì‹œì‘: {payload.cafe_url}")
		print(f"ğŸ” í‚¤ì›Œë“œ: {payload.search_keywords}")
		print(f"ğŸ‘¤ ê²Œì‹œê¸€ ì‘ì„±ì: {payload.post_authors}")
		print(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„±ì: {payload.comment_authors}")
		print(f"ğŸ“Š ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜: {payload.max_articles}")
		
		# ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
		results = await scraper.batch_scraping(
			payload.cafe_url,
			payload.max_pages,
			payload.all_boards,
			payload.selected_boards,
			payload.search_keywords,
			payload.post_authors,
			payload.comment_authors,
			payload.max_articles,
			payload.image_processing,
			payload.period,
			payload.delay_between_requests
		)
		
		# Save to CSV
		csv_paths = []
		successful_results = []
		
		for result in results:
			if "error" not in result:
				csv_path = append_article_bundle_row(OUTPUTS_DIR, result)
				csv_paths.append(csv_path)
				successful_results.append(result)
		
		await scraper.close()
		
		success_count = len(successful_results)
		error_count = len(results) - success_count
		
		return JSONResponse({
			"status": "success",
			"message": f"ë°°ì¹˜ í¬ë¡¤ë§ ì™„ë£Œ: {success_count}ê°œ ê²Œì‹œê¸€ ì²˜ë¦¬ ì„±ê³µ, {error_count}ê°œ ì‹¤íŒ¨",
			"articles_scraped": success_count,
			"articles_failed": error_count,
			"saved_csvs": csv_paths,
			"results": results
		})
		
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤íŒ¨: {str(e)}",
		}, status_code=500)


@app.get("/monitor/status")
async def get_system_status() -> JSONResponse:
	"""ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ"""
	try:
		from app.utils.monitor import performance_monitor
		status = performance_monitor.get_system_status()
		return JSONResponse({
			"status": "success",
			"data": status
		})
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"ì‹œìŠ¤í…œ ìƒíƒœ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
		}, status_code=500)


@app.get("/monitor/stats/{date}")
async def get_daily_stats(date: str) -> JSONResponse:
	"""ì¼ì¼ í†µê³„ ì¡°íšŒ"""
	try:
		from app.utils.monitor import performance_monitor
		stats = performance_monitor.get_daily_stats(date)
		return JSONResponse({
			"status": "success",
			"data": stats
		})
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {str(e)}"
		}, status_code=500)


@app.get("/monitor/cleanup")
async def cleanup_old_metrics(days: int = 30) -> JSONResponse:
	"""ì˜¤ë˜ëœ ë©”íŠ¸ë¦­ íŒŒì¼ ì •ë¦¬"""
	try:
		from app.utils.monitor import performance_monitor
		performance_monitor.cleanup_old_metrics(days)
		return JSONResponse({
			"status": "success",
			"message": f"{days}ì¼ ì´ìƒ ëœ ë©”íŠ¸ë¦­ íŒŒì¼ì´ ì •ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤."
		})
	except Exception as e:
		return JSONResponse({
			"status": "error",
			"message": f"ë©”íŠ¸ë¦­ ì •ë¦¬ ì‹¤íŒ¨: {str(e)}"
		}, status_code=500)

