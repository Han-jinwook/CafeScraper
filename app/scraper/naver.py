from __future__ import annotations
import asyncio
import json
import os
import time
import psutil
import base64
from pathlib import Path
from typing import Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import requests

# ë¡œê¹… ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from app.utils.logger import scraping_logger
except ImportError:
    # ë¡œê¹… ì‹œìŠ¤í…œì´ ì—†ì„ ë•Œ ê¸°ë³¸ ë¡œê¹…
    class DummyLogger:
        def info(self, msg): print(f"INFO: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
        def debug(self, msg): print(f"DEBUG: {msg}")
        def log_scraping_start(self, url, total): self.info(f"ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url} (ì´ {total}ê°œ)")
        def log_scraping_progress(self, current, total, url): self.info(f"ğŸ“„ [{current:3d}/{total:3d}] {url}")
        def log_scraping_success(self, url, title): self.info(f"âœ… ì„±ê³µ: {title}")
        def log_scraping_error(self, url, error): self.error(f"âŒ ì‹¤íŒ¨: {url} - {error}")
        def log_scraping_complete(self, successful, failed, total): self.info(f"ğŸ“Š ì™„ë£Œ: ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
        def log_performance(self, operation, duration, details=""): self.info(f"â±ï¸ {operation}: {duration:.2f}ì´ˆ")
        def log_memory_usage(self, operation, memory_mb): self.info(f"ğŸ’¾ {operation}: {memory_mb:.1f}MB")
        def log_antibot_measure(self, measure, details=""): self.info(f"ğŸ›¡ï¸ ì•ˆí‹°ë´‡ ëŒ€ì‘: {measure}")
    
    scraping_logger = DummyLogger()

class NaverScraper:
    """Naver Cafe scraper using Selenium WebDriver with manual login and cookie persistence."""

    def __init__(self, sessions_dir: str, snapshots_dir: str) -> None:
        self.sessions_dir = Path(sessions_dir)
        self.snapshots_dir = Path(snapshots_dir)
        self.sessions_dir.mkdir(exist_ok=True)
        self.snapshots_dir.mkdir(exist_ok=True)
        
        self.driver: Optional[webdriver.Chrome] = None
        self._cookie_file = self.sessions_dir / "naver_cookies.json"

    def start_browser(self) -> None:
        """Start Chrome browser with persistent context for cookie management."""
        if self.driver:
            print("âœ… ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        try:
            print("ğŸ”„ Selenium Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
            
            # Chrome ì˜µì…˜ ì„¤ì • (ì„¸ì…˜ ì•ˆì •ì„± ê°•í™”)
            chrome_options = Options()
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor")
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-field-trial-config")
            chrome_options.add_argument("--disable-back-forward-cache")
            chrome_options.add_argument("--disable-ipc-flooding-protection")
            # ì„¸ì…˜ ì•ˆì •ì„± ê°•í™” ì˜µì…˜ ì¶”ê°€
            chrome_options.add_argument("--disable-hang-monitor")
            chrome_options.add_argument("--disable-prompt-on-repost")
            chrome_options.add_argument("--disable-sync")
            chrome_options.add_argument("--disable-translate")
            chrome_options.add_argument("--disable-logging")
            chrome_options.add_argument("--disable-permissions-api")
            chrome_options.add_argument("--disable-popup-blocking")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_experimental_option("prefs", {
                "profile.default_content_setting_values.notifications": 2,
                "profile.default_content_settings.popups": 0,
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.cookies": 1,  # ì¿ í‚¤ í—ˆìš©
                "profile.default_content_setting_values.javascript": 1,  # JavaScript í—ˆìš©
                "profile.default_content_setting_values.plugins": 1,  # í”ŒëŸ¬ê·¸ì¸ í—ˆìš©
                "profile.default_content_setting_values.media_stream": 2,  # ë¯¸ë””ì–´ ìŠ¤íŠ¸ë¦¼ ì°¨ë‹¨
                "profile.default_content_setting_values.geolocation": 2,  # ìœ„ì¹˜ ì •ë³´ ì°¨ë‹¨
                "profile.default_content_setting_values.camera": 2,  # ì¹´ë©”ë¼ ì°¨ë‹¨
                "profile.default_content_setting_values.microphone": 2  # ë§ˆì´í¬ ì°¨ë‹¨
            })
            
            # User-Agent ì„¤ì •
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # chrome_options.add_argument("--headless")
            
            # WebDriver ì´ˆê¸°í™”
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Selenium Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            print(f"âŒ ì—ëŸ¬ ìƒì„¸: {str(e)}")
            raise Exception(f"ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

    def _load_cookies(self) -> None:
        """Load saved cookies from file with improved domain handling."""
        if self._cookie_file.exists() and self.driver:
            try:
                with open(self._cookie_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                
                loaded_count = 0
                
                # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™ í›„ ì¿ í‚¤ ë¡œë“œ
                self.driver.get("https://www.naver.com")
                time.sleep(3)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
                
                for cookie in cookies:
                    try:
                        # ë„ë©”ì¸ ìˆ˜ì •í•˜ì—¬ ë¡œë“œ ì‹œë„
                        original_domain = cookie.get('domain', '')
                        
                        # ì¿ í‚¤ ë³µì‚¬ë³¸ ìƒì„±
                        cookie_copy = cookie.copy()
                        
                        # ë„ë©”ì¸ ì²˜ë¦¬ ê°œì„ 
                        if original_domain.startswith('.naver.com'):
                            # .naver.com ë„ë©”ì¸ ì¿ í‚¤ëŠ” ê·¸ëŒ€ë¡œ ë¡œë“œ
                            self.driver.add_cookie(cookie_copy)
                            loaded_count += 1
                        elif original_domain.startswith('.cafe.naver.com'):
                            # .cafe.naver.com ë„ë©”ì¸ ì¿ í‚¤ëŠ” .naver.comìœ¼ë¡œ ë³€ê²½í•˜ì—¬ ë¡œë“œ
                            cookie_copy['domain'] = '.naver.com'
                            self.driver.add_cookie(cookie_copy)
                            loaded_count += 1
                        elif original_domain.startswith('www.naver.com'):
                            # www.naver.com ë„ë©”ì¸ ì¿ í‚¤ëŠ” .naver.comìœ¼ë¡œ ë³€ê²½
                            cookie_copy['domain'] = '.naver.com'
                            self.driver.add_cookie(cookie_copy)
                            loaded_count += 1
                        else:
                            # ê¸°íƒ€ ë„ë©”ì¸ ì¿ í‚¤ëŠ” ë„ë©”ì¸ì„ ì œê±°í•˜ê³  ë¡œë“œ
                            if 'domain' in cookie_copy:
                                del cookie_copy['domain']
                            self.driver.add_cookie(cookie_copy)
                            loaded_count += 1
                            
                    except Exception as e:
                        print(f"âš ï¸ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {cookie.get('name', 'unknown')} - {e}")
                        continue
                
                print(f"âœ… Loaded {loaded_count} cookies from {self._cookie_file}")
                
                # ì¿ í‚¤ ë¡œë“œ í›„ í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨í•˜ì—¬ ì„¸ì…˜ í™œì„±í™”
                self.driver.refresh()
                time.sleep(2)
                
            except Exception as e:
                print(f"âš ï¸ Failed to load cookies: {e}")

    def _save_cookies(self) -> None:
        """Save current cookies to file."""
        if self.driver:
            try:
                cookies = self.driver.get_cookies()
                # ì¿ í‚¤ íŒŒì¼ì´ ì´ë¯¸ ì¡´ì¬í•˜ê³  ìœ íš¨í•œ ê²½ìš° ë®ì–´ì“°ì§€ ì•ŠìŒ
                if self._cookie_file.exists():
                    try:
                        with open(self._cookie_file, 'r', encoding='utf-8') as f:
                            existing_cookies = json.load(f)
                        if len(existing_cookies) > 0:
                            print(f"âœ… ê¸°ì¡´ ì¿ í‚¤ íŒŒì¼ ìœ ì§€ ({len(existing_cookies)}ê°œ ì¿ í‚¤)")
                            return
                    except:
                        pass  # ê¸°ì¡´ íŒŒì¼ì´ ì†ìƒëœ ê²½ìš° ìƒˆë¡œ ì €ì¥
                
                with open(self._cookie_file, 'w', encoding='utf-8') as f:
                    json.dump(cookies, f, ensure_ascii=False, indent=2)
                print(f"âœ… Saved {len(cookies)} cookies to {self._cookie_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to save cookies: {e}")

    def _check_login_status(self) -> bool:
        """ë¡œê·¸ì¸ ìƒíƒœë§Œ í™•ì¸ (ì¬ë¡œê·¸ì¸ ì‹œë„í•˜ì§€ ì•ŠìŒ)"""
        try:
            # ì¿ í‚¤ íŒŒì¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼
            if self._cookie_file.exists():
                print("âœ… ì¿ í‚¤ íŒŒì¼ ì¡´ì¬ - ë¡œê·¸ì¸ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼")
                return True
            
            # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            self.driver.get("https://www.naver.com")
            time.sleep(3)
            
            # ì—¬ëŸ¬ ë°©ë²•ìœ¼ë¡œ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
            login_indicators = [
                # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
                len(self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì¸')]")) == 0,
                # ì‚¬ìš©ì í”„ë¡œí•„ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
                len(self.driver.find_elements(By.XPATH, "//a[contains(@href, 'nid.naver.com')]")) > 0,
                # ë‹‰ë„¤ì„ì´ í‘œì‹œë˜ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
                len(self.driver.find_elements(By.XPATH, "//span[contains(@class, 'MyView-module__link_login___HpHMW')]")) > 0,
                # ë¡œê·¸ì•„ì›ƒ ë²„íŠ¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
                len(self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì•„ì›ƒ')]")) > 0
            ]
            
            # í•˜ë‚˜ë¼ë„ Trueì´ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
            if any(login_indicators):
                print("âœ… ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ë¨")
                return True
            else:
                print("âŒ ë¡œê·¸ì¸ë˜ì§€ ì•ŠìŒ")
                return False
        except Exception as e:
            print(f"âŒ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì‹¤íŒ¨: {e}")
            return False

    def manual_login(self) -> bool:
        """ìˆ˜ë™ ë¡œê·¸ì¸ í”„ë¡œì„¸ìŠ¤ - ì‚¬ìš©ìê°€ ë¸Œë¼ìš°ì €ì—ì„œ ì§ì ‘ ë¡œê·¸ì¸"""
        if not self.driver:
            self.start_browser()
        
        # ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™
        print("ğŸŒ ë„¤ì´ë²„ ë¡œê·¸ì¸ í˜ì´ì§€ë¡œ ì´ë™ ì¤‘...")
        self.driver.get("https://nid.naver.com/nidlogin.login")
        time.sleep(3)
        
        print("ğŸ” ë¸Œë¼ìš°ì €ì—ì„œ ë„¤ì´ë²„ì— ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
        print("   ë¡œê·¸ì¸ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ê°ì§€ë©ë‹ˆë‹¤...")
        
        # ìë™ ë¡œê·¸ì¸ ê°ì§€ (ìµœëŒ€ 5ë¶„ ëŒ€ê¸°)
        max_wait_time = 120  # 2ë¶„
        check_interval = 3   # 3ì´ˆë§ˆë‹¤ í™•ì¸
        waited_time = 0
        
        while waited_time < max_wait_time:
            try:
                time.sleep(check_interval)
                waited_time += check_interval
                
                # ë¸Œë¼ìš°ì € ì°½ì´ ë‹«í˜”ëŠ”ì§€ í™•ì¸
                try:
                    current_url = self.driver.current_url
                except Exception as e:
                    print(f"âŒ ë¸Œë¼ìš°ì € ì°½ì´ ë‹«í˜”ìŠµë‹ˆë‹¤: {e}")
                    print("ğŸ” ë¸Œë¼ìš°ì €ì—ì„œ ë„¤ì´ë²„ì— ë¡œê·¸ì¸í•´ì£¼ì„¸ìš”.")
                    print("   ë¡œê·¸ì¸ ì™„ë£Œ í›„ ìë™ìœ¼ë¡œ ê°ì§€ë©ë‹ˆë‹¤...")
                    # ë¸Œë¼ìš°ì € ì¬ì‹œì‘
                    self.start_browser()
                    self.driver.get("https://nid.naver.com/nidlogin.login")
                    time.sleep(3)
                    continue
                
                # ë¡œê·¸ì¸ ì„±ê³µ í™•ì¸ (ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ë¦¬ë‹¤ì´ë ‰íŠ¸ë¨)
                if "naver.com" in current_url and "nidlogin" not in current_url:
                    self._save_cookies()
                    print("âœ… ë¡œê·¸ì¸ ì„±ê³µ! ì¿ í‚¤ê°€ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    return True
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                remaining_time = max_wait_time - waited_time
                print(f"â³ ë¡œê·¸ì¸ ëŒ€ê¸° ì¤‘... ({remaining_time}ì´ˆ ë‚¨ìŒ)")
                
            except Exception as e:
                print(f"âš ï¸ ë¡œê·¸ì¸ í™•ì¸ ì¤‘ ì˜¤ë¥˜: {e}")
                continue
        
        print("âŒ ë¡œê·¸ì¸ ì‹œê°„ ì´ˆê³¼. ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.")
        return False

    def ensure_logged_in(self) -> bool:
        """Ensure user is logged in to Naver. Returns True if successful."""
        if not self.driver:
            self.start_browser()
        
        # ë¸Œë¼ìš°ì € ì„¸ì…˜ ìœ íš¨ì„± í™•ì¸
        try:
            current_url = self.driver.current_url
            if not current_url or current_url == "data:,":
                print("âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠì–´ì§ - ì¬ì‹œì‘ ì¤‘...")
                self.start_browser()
        except Exception as e:
            print(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ í™•ì¸ ì‹¤íŒ¨: {e} - ì¬ì‹œì‘ ì¤‘...")
            self.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        self._load_cookies()
        
        # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
        self.driver.get("https://www.naver.com")
        time.sleep(3)  # ë¡œë”© ëŒ€ê¸° ì‹œê°„ ì¦ê°€
        
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        try:
            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
            login_button = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì¸')]")
            if not login_button:
                print("âœ… Already logged in to Naver")
                self._save_cookies()
                return True
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸ í•„ìš”
            print("ğŸ” Manual login required. Please log in to Naver in the browser window.")
            print("   The system will automatically detect when you're logged in...")
            
            # ìë™ ë¡œê·¸ì¸ ê°ì§€ (ìµœëŒ€ 5ë¶„ ëŒ€ê¸°)
            max_wait_time = 120  # 2ë¶„
            check_interval = 3   # 3ì´ˆë§ˆë‹¤ í™•ì¸
            waited_time = 0
            
            while waited_time < max_wait_time:
                time.sleep(check_interval)
                waited_time += check_interval
                
                # ë¸Œë¼ìš°ì € ì„¸ì…˜ ìœ íš¨ì„± ì¬í™•ì¸
                try:
                    current_url = self.driver.current_url
                    if not current_url or current_url == "data:,":
                        print("âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠì–´ì§ - ì¬ì‹œì‘ ì¤‘...")
                        self.start_browser()
                        self._load_cookies()
                        self.driver.get("https://www.naver.com")
                        time.sleep(3)
                except Exception as e:
                    print(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ í™•ì¸ ì‹¤íŒ¨: {e} - ì¬ì‹œì‘ ì¤‘...")
                    self.start_browser()
                    self._load_cookies()
                    self.driver.get("https://www.naver.com")
                    time.sleep(3)
                
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                self.driver.refresh()
                time.sleep(2)
                
                # ë¡œê·¸ì¸ ìƒíƒœ ì¬í™•ì¸
                login_button_after = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì¸')]")
                if not login_button_after:
                    self._save_cookies()
                    print("âœ… Login successful! Cookies saved.")
                    return True
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                remaining_time = max_wait_time - waited_time
                print(f"â³ Waiting for login... ({remaining_time}s remaining)")
            
            print("âŒ Login timeout. Please try again.")
            return False
                
        except Exception as e:
            print(f"âŒ Login check failed: {e}")
            return False

    def scrape_article(self, url: str, include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None, max_retries: int = 3):
        """Scrape a single article with comments and images."""
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ì„ ê°„ì†Œí™” (ì´ë¯¸ ê²Œì‹œíŒ ì¡°íšŒì—ì„œ í™•ì¸ë¨)
        if not self.driver:
            raise Exception("Browser not started")
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì‹œë„ {attempt + 1}/{max_retries}: {url}")
                
                # Navigate to article with retry logic
                self._navigate_with_retry(url, max_retries=2)
                
                # JavaScript ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
                print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
                time.sleep(30)
                
                # Take snapshot for debugging
                # URLì—ì„œ ì•ˆì „í•œ ë””ë ‰í„°ë¦¬ëª… ìƒì„±
                import re
                safe_name = re.sub(r'[^\w\-_.]', '_', url.split('/')[-1].split('?')[0])
                snapshot_dir = self.snapshots_dir / safe_name
                snapshot_dir.mkdir(exist_ok=True)
                self.driver.save_screenshot(str(snapshot_dir / f"page_attempt_{attempt + 1}.png"))
                
                # Extract article information
                article_data = self._extract_article_data(url)
                
                # Extract images and convert to base64
                images_base64 = self._extract_images()
                
                # Extract comments with filtering
                comments = self._extract_comments(include_nicks, exclude_nicks)
                
                # Combine all data
                result = {
                    **article_data,
                    "images_base64": images_base64,
                    "comments": comments,
                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                
                print(f"âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {article_data.get('title', 'N/A')}")
                return result
                
            except Exception as e:
                print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                    print(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
                    raise e

    def _extract_article_data(self, url: str) -> dict:
        """Extract basic article information."""
        try:
            # Extract cafe ID from URL
            cafe_id = url.split("cafe.naver.com/")[1].split("/")[0] if "cafe.naver.com" in url else "unknown"
            
            # Extract article ID from URL
            article_id = url.split("/")[-1] if "/" in url else "unknown"
            
            # ë„¤ì´ë²„ ì¹´í˜ ìµœì‹  êµ¬ì¡°ì— ë§ëŠ” ìƒˆë¡œìš´ ì…€ë ‰í„° (2025ë…„ ì—…ë°ì´íŠ¸)
            title_selectors = [
                # 1. ì‹¤ì œ ë°œê²¬ëœ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„)
                "h3.title",
                "h2.title", 
                "h1.title",
                ".title",
                # 2. ë„¤ì´ë²„ ì¹´í˜ ìµœì‹  êµ¬ì¡° (2024ë…„ ê¸°ì¤€)
                ".se-title-text",
                ".se-fs-",
                ".se-component-content h1",
                ".se-component-content h2", 
                ".se-component-content h3",
                ".se-text-paragraph",
                # 3. ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ë‚´ ì œëª©
                ".article_content h1",
                ".article_content h2",
                ".article_content h3",
                ".post_content h1",
                ".post_content h2",
                ".post_content h3",
                # 4. ì¼ë°˜ì ì¸ ì œëª© ì…€ë ‰í„°
                ".article_title",
                ".post_title",
                ".content_title",
                ".view_title",
                # 5. ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™” ì…€ë ‰í„°
                ".cafe-article-title",
                ".article-view-title",
                ".post-view-title",
                # 6. ìµœì‹  ë„¤ì´ë²„ ì¹´í˜ êµ¬ì¡°
                ".Layout_content__pUOz1 h1",
                ".Layout_content__pUOz1 h2",
                ".Layout_content__pUOz1 h3",
                # 7. ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ - ëª¨ë“  h íƒœê·¸ì—ì„œ ì¹´í˜ ì œëª© ì œì™¸
                "h1:not([class*='Layout_cafe_name']):not([class*='Header'])",
                "h2:not([class*='Layout_cafe_name']):not([class*='Header'])",
                "h3:not([class*='Layout_cafe_name']):not([class*='Header'])",
                # 8. ê²Œì‹œê¸€ ì œëª©ì´ ìˆì„ ìˆ˜ ìˆëŠ” íŠ¹ì • ì˜ì—­ë“¤
                "[class*='article'] h1",
                "[class*='article'] h2", 
                "[class*='article'] h3",
                "[class*='post'] h1",
                "[class*='post'] h2",
                "[class*='post'] h3",
                "[class*='content'] h1",
                "[class*='content'] h2",
                "[class*='content'] h3"
            ]
            
            title = self._safe_extract(title_selectors, default="ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë””ë²„ê¹…: ì œëª© ì¶”ì¶œ ì‹¤íŒ¨ ì‹œ í˜ì´ì§€ êµ¬ì¡° ë¶„ì„
            if title == "ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ" or "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" in title:
                print("ğŸ” ë””ë²„ê¹…: ì œëª© ì¶”ì¶œ ë¬¸ì œ ë¶„ì„ ì¤‘...")
                try:
                    # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ê°€ëŠ¥í•œ ì œëª© ìš”ì†Œë“¤ ì°¾ê¸°
                    page_source = self.driver.page_source
                    
                    # h1, h2, h3 íƒœê·¸ë“¤ ì°¾ê¸°
                    import re
                    h_tags = re.findall(r'<h[1-3][^>]*>([^<]+)</h[1-3]>', page_source)
                    if h_tags:
                        print(f"ğŸ” ë°œê²¬ëœ h íƒœê·¸ë“¤: {h_tags[:5]}")  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
                    
                    # title ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
                    title_classes = re.findall(r'class="([^"]*title[^"]*)"', page_source)
                    if title_classes:
                        print(f"ğŸ” ë°œê²¬ëœ title í´ë˜ìŠ¤ë“¤: {title_classes[:5]}")
                    
                    # se- ê´€ë ¨ í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
                    se_classes = re.findall(r'class="([^"]*se-[^"]*)"', page_source)
                    if se_classes:
                        print(f"ğŸ” ë°œê²¬ëœ se- í´ë˜ìŠ¤ë“¤: {se_classes[:5]}")
                    
                    # ê²Œì‹œê¸€ ì œëª©ì´ ìˆì„ ìˆ˜ ìˆëŠ” ì˜ì—­ë“¤ í™•ì¸
                    print("ğŸ” ê²Œì‹œê¸€ ì œëª© ì˜ì—­ í™•ì¸ ì¤‘...")
                    try:
                        # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ë‚´ ì œëª© ì°¾ê¸°
                        from selenium.webdriver.common.by import By
                        content_area = self.driver.find_elements(By.CSS_SELECTOR, ".article_content, .post_content, .se-main-container")
                        if content_area:
                            print(f"âœ… ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ë°œê²¬: {len(content_area)}ê°œ")
                            for i, area in enumerate(content_area[:2]):  # ì²˜ìŒ 2ê°œë§Œ í™•ì¸
                                try:
                                    titles_in_area = area.find_elements(By.CSS_SELECTOR, "h1, h2, h3, .title")
                                    if titles_in_area:
                                        print(f"   ì˜ì—­ {i+1} ë‚´ ì œëª© ìš”ì†Œ: {len(titles_in_area)}ê°œ")
                                        for j, title_elem in enumerate(titles_in_area[:3]):  # ì²˜ìŒ 3ê°œë§Œ í™•ì¸
                                            try:
                                                title_text = title_elem.text.strip()
                                                if title_text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in title_text:
                                                    print(f"     ì œëª© {j+1}: {title_text[:50]}...")
                                            except:
                                                pass
                                except:
                                    pass
                        else:
                            print("âŒ ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                        
                        # ì™„ì „íˆ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ - ëª¨ë“  h íƒœê·¸ í™•ì¸
                        print("ğŸ” ëª¨ë“  h íƒœê·¸ í™•ì¸ ì¤‘...")
                        all_h_tags = self.driver.find_elements(By.CSS_SELECTOR, "h1, h2, h3, h4, h5, h6")
                        if all_h_tags:
                            print(f"âœ… ì „ì²´ h íƒœê·¸ ë°œê²¬: {len(all_h_tags)}ê°œ")
                            for i, h_tag in enumerate(all_h_tags[:10]):  # ì²˜ìŒ 10ê°œë§Œ í™•ì¸
                                try:
                                    h_text = h_tag.text.strip()
                                    h_class = h_tag.get_attribute("class") or ""
                                    if h_text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in h_text:
                                        print(f"   h íƒœê·¸ {i+1}: {h_text[:50]}... (í´ë˜ìŠ¤: {h_class[:30]})")
                                except:
                                    pass
                        
                        # ê²Œì‹œê¸€ ì œëª©ì´ ìˆì„ ìˆ˜ ìˆëŠ” íŠ¹ì • ì˜ì—­ë“¤ í™•ì¸
                        print("ğŸ” ê²Œì‹œê¸€ ì œëª© íŠ¹í™” ì˜ì—­ í™•ì¸ ì¤‘...")
                        title_areas = [
                            ".article_title",
                            ".post_title", 
                            ".content_title",
                            ".view_title",
                            ".se-title-text",
                            ".se-fs-",
                            ".cafe-article-title",
                            ".article-view-title",
                            ".post-view-title",
                            ".content-view-title"
                        ]
                        
                        for area_selector in title_areas:
                            try:
                                area_elements = self.driver.find_elements(By.CSS_SELECTOR, area_selector)
                                if area_elements:
                                    print(f"âœ… {area_selector} ì˜ì—­ ë°œê²¬: {len(area_elements)}ê°œ")
                                    for i, elem in enumerate(area_elements[:3]):
                                        try:
                                            elem_text = elem.text.strip()
                                            if elem_text and "ë¹„íƒ€ë¯¼Dìì™¸ì„ ìš”ë²•" not in elem_text:
                                                print(f"   {area_selector} {i+1}: {elem_text[:50]}...")
                                        except:
                                            pass
                            except:
                                pass
                        
                    except Exception as e:
                        print(f"âŒ ê²Œì‹œê¸€ ì œëª© ì˜ì—­ í™•ì¸ ì˜¤ë¥˜: {e}")
                        
                except Exception as e:
                    print(f"âš ï¸ ì œëª© ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {e}")
            
            # Try multiple selectors for author (updated for current Naver Cafe structure)
            author_selectors = [
                # ìµœì‹  ë„¤ì´ë²„ ì¹´í˜ êµ¬ì¡°
                ".nick",
                ".nickname", 
                ".author",
                ".writer",
                ".user_nick",
                ".user_name",
                ".member_nick",
                ".member_name",
                # ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™” ì…€ë ‰í„°
                ".cafe-nick",
                ".cafe-author",
                ".article-author",
                ".post-author",
                # ì¼ë°˜ì ì¸ ì…€ë ‰í„°
                "[data-testid='author']",
                ".se-fs-",
                ".nickname_text",
                ".author_name",
                ".writer_name",
                "[class*='nick']",
                "[class*='author']",
                "[class*='writer']",
                ".user_info .nick",
                ".user_info .nickname",
                ".article_info .nick",
                ".article_info .nickname",
                # ì¶”ê°€ ì‹œë„
                "span[class*='nick']",
                "div[class*='author']",
                "div[class*='writer']"
            ]
            
            author = self._safe_extract(author_selectors, default="ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # ë„¤ì´ë²„ ì¹´í˜ ìµœì‹  êµ¬ì¡°ì— ë§ëŠ” ë‚´ìš© ì…€ë ‰í„° (2025ë…„ ì—…ë°ì´íŠ¸)
            content_selectors = [
                # 1. ì‹¤ì œ ë°œê²¬ëœ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„)
                ".content",
                # 2. ë„¤ì´ë²„ ì—ë””í„° ìµœì‹  êµ¬ì¡°
                ".se-main-container",
                ".se-component-content",
                ".se-text-paragraph",
                ".se-text",
                ".se-component",
                # 3. ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­
                ".article_content",
                ".post_content",
                ".article_text",
                ".board_text",
                ".article_body",
                # 4. ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™” ì…€ë ‰í„°
                ".cafe-content",
                ".cafe-article-content",
                ".article-view-content",
                ".view-content",
                ".content-view",
                # 5. ìµœì‹  ë„¤ì´ë²„ ì¹´í˜ êµ¬ì¡°
                ".Layout_content__pUOz1",
                ".Layout_content__pUOz1 .se-main-container",
                ".Layout_content__pUOz1 .se-component-content",
                # 6. ì¼ë°˜ì ì¸ ì…€ë ‰í„°
                "[data-testid='article-content']",
                "[class*='content']",
                "[class*='Content']",
                "[class*='text']",
                "[class*='Text']",
                # 7. se- ê´€ë ¨ ëª¨ë“  í´ë˜ìŠ¤
                "div[class*='se-']",
                "p[class*='se-']",
                "span[class*='se-']",
                # 8. ê²Œì‹œê¸€ ë³¸ë¬¸ì´ ìˆì„ ìˆ˜ ìˆëŠ” ì˜ì—­ë“¤
                "div[class*='article']",
                "div[class*='post']",
                "div[class*='content']"
            ]
            
            content_text = self._safe_extract(content_selectors, default="ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            content_html = self._safe_extract_html(content_selectors, default="<p>ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ</p>")
            
            # ë””ë²„ê¹…: í˜ì´ì§€ êµ¬ì¡° í™•ì¸
            if content_text == "ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ":
                print("ğŸ” ë””ë²„ê¹…: í˜ì´ì§€ êµ¬ì¡° ë¶„ì„ ì¤‘...")
                try:
                    # í˜ì´ì§€ ì†ŒìŠ¤ì—ì„œ ê°€ëŠ¥í•œ ì…€ë ‰í„° ì°¾ê¸°
                    page_source = self.driver.page_source
                    if "se-main-container" in page_source:
                        print("âœ… se-main-container ë°œê²¬ë¨")
                    if "article" in page_source.lower():
                        print("âœ… article ê´€ë ¨ í´ë˜ìŠ¤ ë°œê²¬ë¨")
                    if "content" in page_source.lower():
                        print("âœ… content ê´€ë ¨ í´ë˜ìŠ¤ ë°œê²¬ë¨")
                    
                    # ì‹¤ì œ ì¡´ì¬í•˜ëŠ” í´ë˜ìŠ¤ë“¤ ì°¾ê¸°
                    import re
                    class_pattern = r'class="([^"]*)"'
                    classes = re.findall(class_pattern, page_source)
                    content_classes = [cls for cls in classes if any(keyword in cls.lower() for keyword in ['content', 'article', 'text', 'se-'])]
                    if content_classes:
                        print(f"ğŸ” ë°œê²¬ëœ í´ë˜ìŠ¤ë“¤: {content_classes[:10]}")  # ì²˜ìŒ 10ê°œë§Œ ì¶œë ¥
                except Exception as e:
                    print(f"âš ï¸ ë””ë²„ê¹… ì¤‘ ì˜¤ë¥˜: {e}")
            
            # Try to extract date
            date_selectors = [
                ".date",
                ".time", 
                ".created_at",
                "[data-testid='created-date']",
                ".article_date",
                ".post_date",
                ".board_date",
                "[class*='date']",
                "[class*='Date']",
                "[class*='time']",
                "[class*='Time']",
                ".article_info .date",
                ".article_info .time",
                ".user_info .date",
                ".user_info .time"
            ]
            
            posted_at = self._safe_extract(date_selectors, default=None)
            if posted_at == "ì•Œ ìˆ˜ ì—†ìŒ":
                posted_at = None
            
            return {
                "cafe_id": cafe_id,
                "article_id": article_id,
                "article_url": url,
                "title": title,
                "author_nickname": author,
                "posted_at": posted_at,
                "content_text": content_text,
                "content_html": content_html
            }
            
        except Exception as e:
            print(f"âš ï¸ Error extracting article data: {e}")
            return {
                "cafe_id": "unknown",
                "article_id": "unknown",
                "article_url": url,
                "title": "ì˜¤ë¥˜ ë°œìƒ",
                "author_nickname": "ì•Œ ìˆ˜ ì—†ìŒ",
                "posted_at": None,
                "content_text": f"ì˜¤ë¥˜: {str(e)}",
                "content_html": f"<p>ì˜¤ë¥˜: {str(e)}</p>"
            }

    def _extract_images(self, max_images: int = 10, max_size_mb: float = 5.0) -> list:
        """Extract images and convert to base64 with memory optimization."""
        images = []
        try:
            # Find all images in the article
            image_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            # Limit number of images to prevent memory issues
            if len(image_elements) > max_images:
                print(f"âš ï¸ Too many images ({len(image_elements)}), limiting to {max_images}")
                image_elements = image_elements[:max_images]
            
            for i, img in enumerate(image_elements):
                try:
                    # Get image source
                    src = img.get_attribute("src")
                    if not src or src.startswith("data:"):
                        continue
                    
                    # Download image and convert to base64
                    response = requests.get(src, timeout=10)
                    if response.status_code == 200:
                        image_data = response.content
                        
                        # Check image size to prevent memory issues
                        size_mb = len(image_data) / (1024 * 1024)
                        if size_mb > max_size_mb:
                            print(f"âš ï¸ Image {i+1} too large ({size_mb:.1f}MB), skipping")
                            continue
                        
                        base64_data = base64.b64encode(image_data).decode('utf-8')
                        
                        # Determine MIME type
                        mime_type = "image/jpeg"
                        if src.lower().endswith('.png'):
                            mime_type = "image/png"
                        elif src.lower().endswith('.gif'):
                            mime_type = "image/gif"
                        
                        images.append({
                            "mime": mime_type,
                            "data": base64_data,
                            "filename": f"image_{i+1}.jpg",
                            "size_mb": round(size_mb, 2)
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ Error processing image {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting images: {e}")
        
        return images

    def _extract_comments(self, include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None) -> list:
        """Extract comments with nickname filtering."""
        comments = []
        try:
            # Try multiple selectors for comments (updated for current Naver Cafe structure)
            comment_selectors = [
                # 1. ì‹¤ì œ ë°œê²¬ëœ ì…€ë ‰í„°ë“¤ (ìš°ì„ ìˆœìœ„)
                ".comment_area",
                ".LinkComment",
                # 2. ìµœì‹  ë„¤ì´ë²„ ì¹´í˜ êµ¬ì¡°
                ".comment",
                ".reply", 
                ".comment_item",
                ".reply_item",
                ".cafe-comment",
                ".cafe-reply",
                ".article-comment",
                ".article-reply",
                # 3. ë„¤ì´ë²„ ì¹´í˜ íŠ¹í™” ì…€ë ‰í„°
                ".comment-list .comment",
                ".comment-list .reply",
                ".reply-list .comment",
                ".reply-list .reply",
                ".comment-area .comment",
                ".comment-area .reply",
                ".reply-area .comment",
                ".reply-area .reply",
                # 4. ì¼ë°˜ì ì¸ ì…€ë ‰í„°
                "[data-testid='comment']",
                "[class*='comment']",
                "[class*='Comment']",
                "[class*='reply']",
                "[class*='Reply']",
                # 5. ì¶”ê°€ ì‹œë„
                "div[class*='comment']",
                "div[class*='reply']",
                "li[class*='comment']",
                "li[class*='reply']"
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        comment_elements = elements
                        break
                except:
                    continue
            
            for comment_element in comment_elements:
                try:
                    # Extract comment text
                    text_element = comment_element.find_elements(By.CSS_SELECTOR, ".comment_text, .reply_text, .content")
                    text = text_element[0].text if text_element else "ëŒ“ê¸€ ë‚´ìš© ì—†ìŒ"
                    
                    # Extract author
                    author_element = comment_element.find_elements(By.CSS_SELECTOR, ".nick, .nickname, .author")
                    author = author_element[0].text if author_element else "ì•Œ ìˆ˜ ì—†ìŒ"
                    
                    # Extract date
                    date_element = comment_element.find_elements(By.CSS_SELECTOR, ".date, .time")
                    date = date_element[0].text if date_element else None
                    
                    # Apply nickname filtering
                    should_include = True
                    
                    if include_nicks and include_nicks:
                        should_include = any(nick in author for nick in include_nicks)
                    
                    if exclude_nicks and exclude_nicks:
                        should_include = should_include and not any(nick in author for nick in exclude_nicks)
                    
                    if should_include:
                        comments.append({
                            "comment_id": f"comment_{len(comments)+1}",
                            "nickname": author.strip() if author else "ì•Œ ìˆ˜ ì—†ìŒ",
                            "text": text.strip() if text else "ëŒ“ê¸€ ë‚´ìš© ì—†ìŒ",
                            "created_at": date.strip() if date else None
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ Error processing comment: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting comments: {e}")
        
        return comments

    def scrape_board_articles(self, board_url: str, max_pages: int = 5) -> list[dict]:
        """Scrape articles from a board page with pagination."""
        # ê°„ë‹¨í•œ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        if not self._cookie_file.exists():
            raise Exception("Login required but failed")
        
        articles = []
        page = 1
        total_articles = 0
        
        print(f"ğŸ“Š ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {board_url}")
        print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print("=" * 50)
        
        while page <= max_pages:
            try:
                progress = f"[í˜ì´ì§€ {page:2d}/{max_pages:2d}]"
                print(f"ğŸ“„ {progress} ê²Œì‹œíŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                
                # Navigate to board page
                page_url = f"{board_url}?page={page}" if "?" not in board_url else f"{board_url}&page={page}"
                self.driver.get(page_url)
                time.sleep(2)
                
                # Take snapshot for debugging
                snapshot_dir = self.snapshots_dir / f"board_page_{page}"
                snapshot_dir.mkdir(exist_ok=True)
                self.driver.save_screenshot(str(snapshot_dir / "page.png"))
                
                # Extract article links from current page
                page_articles = self._extract_article_links_from_board()
                
                if not page_articles:
                    print(f"ğŸ“„ {progress} ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨")
                    break
                
                articles.extend(page_articles)
                total_articles += len(page_articles)
                print(f"âœ… {progress} ì™„ë£Œ - ë°œê²¬ëœ ê²Œì‹œê¸€: {len(page_articles)}ê°œ (ëˆ„ì : {total_articles}ê°œ)")
                
                page += 1
                
                # Add delay to avoid being blocked
                time.sleep(2)
                
            except Exception as e:
                print(f"âš ï¸ {progress} ì˜¤ë¥˜: {e}")
                break
        
        print("=" * 50)
        print(f"ğŸ“Š ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {total_articles}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        return articles

    def _extract_article_links_from_board(self) -> list[dict]:
        """Extract article links and basic info from board page."""
        articles = []
        
        try:
            # Try multiple selectors for article links
            article_selectors = [
                "a[href*='/ArticleRead.nhn']",
                "a[href*='/ArticleRead']", 
                ".article",
                ".board_list a",
                ".list_item a",
                "tr td a"
            ]
            
            article_links = []
            for selector in article_selectors:
                try:
                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if links:
                        article_links = links
                        print(f"âœ… Found {len(links)} article links using selector: {selector}")
                        break
                except:
                    continue
            
            if not article_links:
                print("âš ï¸ No article links found on this page")
                return articles
            
            print(f"ğŸ” Processing {len(article_links)} links...")
            valid_links = 0
            
            for i, link in enumerate(article_links):
                try:
                    # Get article URL
                    href = link.get_attribute("href")
                    if not href:
                        continue
                    
                    # Debug: Print first few links
                    if i < 5:
                        print(f"ğŸ”— Link {i+1}: {href}")
                    
                    # Check for various article URL patterns
                    if not any(pattern in href for pattern in ["ArticleRead", "ArticleRead.nhn", "articleid", "clubid", "articles"]):
                        continue
                    
                    valid_links += 1
                    
                    # Make URL absolute
                    if href.startswith("/"):
                        href = f"https://cafe.naver.com{href}"
                    elif not href.startswith("http"):
                        href = f"https://cafe.naver.com/{href}"
                    
                    # Extract basic info from link text
                    title = link.text.strip() if link.text else "ì œëª© ì—†ìŒ"
                    
                    # Try to find author and date from parent elements
                    parent_row = link.find_element(By.XPATH, "ancestor::tr")
                    author = "ì•Œ ìˆ˜ ì—†ìŒ"
                    date = None
                    
                    if parent_row:
                        # Try to find author in the same row
                        author_selectors = [".nick", ".nickname", ".author", "td:nth-child(2)", "td:nth-child(3)"]
                        for author_sel in author_selectors:
                            try:
                                author_elem = parent_row.find_elements(By.CSS_SELECTOR, author_sel)
                                if author_elem:
                                    author_text = author_elem[0].text
                                    if author_text and author_text.strip():
                                        author = author_text.strip()
                                        break
                            except:
                                continue
                        
                        # Try to find date in the same row
                        date_selectors = [".date", ".time", "td:last-child", "td:nth-last-child(2)"]
                        for date_sel in date_selectors:
                            try:
                                date_elem = parent_row.find_elements(By.CSS_SELECTOR, date_sel)
                                if date_elem:
                                    date_text = date_elem[0].text
                                    if date_text and date_text.strip():
                                        date = date_text.strip()
                                        break
                            except:
                                continue
                    
                    # Extract article ID from URL
                    article_id = href.split("/")[-1] if "/" in href else "unknown"
                    
                    articles.append({
                        "article_id": article_id,
                        "article_url": href,
                        "title": title,
                        "author_nickname": author,
                        "posted_at": date,
                        "scraped_at": None  # Will be filled when scraping full article
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ Error processing article link: {e}")
                    continue
            
            print(f"âœ… Found {valid_links} valid article links out of {len(article_links)} total links")
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting article links: {e}")
        
        return articles

    def scrape_multiple_articles(self, article_urls: list[str], include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None, max_concurrent: int = 3) -> list[dict]:
        """Scrape multiple articles with progress tracking."""
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ì„ ê°„ì†Œí™” (ì´ë¯¸ ê²Œì‹œíŒ ì¡°íšŒì—ì„œ í™•ì¸ë¨)
        if not self.driver:
            raise Exception("Browser not started")
        
        start_time = time.time()
        results = []
        total = len(article_urls)
        successful = 0
        failed = 0
        
        # ë¡œê¹… ì‹œì‘
        scraping_logger.log_scraping_start("ë‹¤ì¤‘ ê²Œì‹œê¸€", total)
        scraping_logger.log_performance("ë™ì‹œ ì²˜ë¦¬ ì„¤ì •", 0, f"ìµœëŒ€ {max_concurrent}ê°œ")
        
        print(f"ğŸš€ ë‹¤ì¤‘ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {total}ê°œ ê²Œì‹œê¸€")
        print("=" * 60)
        
        # Process articles sequentially (Selenium doesn't support true concurrency)
        for i, url in enumerate(article_urls, 1):
            try:
                progress = f"[{i:3d}/{total:3d}]"
                percentage = (i / total) * 100
                print(f"ğŸ“„ {progress} ({percentage:5.1f}%) ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
                
                # Scrape individual article
                article_data = self.scrape_article(url, include_nicks, exclude_nicks)
                results.append(article_data)
                successful += 1
                print(f"âœ… {progress} ì™„ë£Œ")
                
                # Add delay between requests
                if i < total:
                    delay = self._calculate_delay(i, total)
                    scraping_logger.log_antibot_measure("ìš”ì²­ ê°„ ëŒ€ê¸°", f"{delay}ì´ˆ")
                    time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ {progress} ì‹¤íŒ¨: {e}")
                failed += 1
                results.append({
                    "article_url": url,
                    "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                    "error": str(e),
                    "scraped_at": None
                })
        
        print("=" * 60)
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {total}ê°œ ì¤‘ ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
        return results

    def _navigate_with_retry(self, url: str, max_retries: int = 3) -> None:
        """Navigate to URL with retry logic."""
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                time.sleep(3)  # Wait for page to load
                return
            except Exception as e:
                print(f"âš ï¸ ë„¤ë¹„ê²Œì´ì…˜ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    raise e

    def _safe_extract(self, selectors: list[str], timeout: int = 2, default: str = "ì•Œ ìˆ˜ ì—†ìŒ") -> str:
        """Safely extract text using multiple selectors."""
        for selector in selectors:
            try:
                element = WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                if element and element.text.strip():
                    return element.text.strip()
            except:
                continue
        return default

    def _safe_extract_html(self, selectors: list[str], timeout: int = 2, default: str = "<p>ì•Œ ìˆ˜ ì—†ìŒ</p>") -> str:
        """Safely extract HTML using multiple selectors."""
        for selector in selectors:
            try:
                element = WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                if element and element.get_attribute("innerHTML").strip():
                    return element.get_attribute("innerHTML").strip()
            except:
                continue
        return default

    def _calculate_delay(self, current: int, total: int) -> float:
        """ë™ì  ì§€ì—° ì‹œê°„ ê³„ì‚° - ì•ˆí‹°ë´‡ ëŒ€ì‘"""
        import random
        
        # ê¸°ë³¸ ì§€ì—° ì‹œê°„ (2-5ì´ˆ)
        base_delay = random.uniform(2.0, 5.0)
        
        # ì§„í–‰ë¥ ì— ë”°ë¥¸ ì¶”ê°€ ì§€ì—° (í›„ë°˜ë¶€ì¼ìˆ˜ë¡ ë” ì˜¤ë˜ ëŒ€ê¸°)
        progress_factor = current / total
        additional_delay = progress_factor * 3.0  # ìµœëŒ€ 3ì´ˆ ì¶”ê°€
        
        # ëœë¤ ìš”ì†Œ ì¶”ê°€
        random_factor = random.uniform(0.5, 1.5)
        
        total_delay = (base_delay + additional_delay) * random_factor
        
        # ìµœëŒ€ 10ì´ˆë¡œ ì œí•œ
        return min(total_delay, 10.0)
    
    def _get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            return memory_info.rss / (1024 * 1024)  # MB ë‹¨ìœ„
        except:
            return 0.0

    def get_cafe_boards(self, cafe_url: str) -> list[dict]:
        """ì¹´í˜ì˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‹œì‘
            if not self.driver:
                print("ğŸ”„ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
                self.start_browser()
            
            # ì¿ í‚¤ ë¡œë“œ
            self._load_cookies()
            
            # ì¿ í‚¤ê°€ ìˆìœ¼ë©´ ë¡œê·¸ì¸ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¹´í˜ ì ‘ì† ì‹œë„
            if self._cookie_file.exists():
                print("âœ… ì €ì¥ëœ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í˜ ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ ì €ì¥ëœ ì¿ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ í•´ê²° ë°©ë²•: ì›¹ UIì—ì„œ 'ë¡œê·¸ì¸ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                # ì¿ í‚¤ê°€ ì—†ì–´ë„ ì¼ë‹¨ ì‹œë„í•´ë³´ê¸° (batch_scrapingì—ì„œ ì´ë¯¸ ì²˜ë¦¬ë¨)
                print("ğŸ”„ ì¿ í‚¤ ì—†ì´ ì¹´í˜ ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            
            # ì¹´í˜ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            print(f"ğŸŒ ì¹´í˜ í˜ì´ì§€ ì´ë™: {cafe_url}")
            self.driver.get(cafe_url)
            time.sleep(3)
            
            # ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ
            boards = self._extract_cafe_boards()
            
            scraping_logger.log_scraping_success(cafe_url, f"ê²Œì‹œíŒ {len(boards)}ê°œ ì¡°íšŒ")
            return boards
            
        except Exception as e:
            scraping_logger.log_scraping_error(cafe_url, str(e))
            raise e
    
    def _extract_cafe_boards(self) -> list[dict]:
        """ì¹´í˜ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ"""
        boards = []
        
        try:
            # ê²Œì‹œíŒ ë§í¬ ì„ íƒìë“¤ (ë” í¬ê´„ì ìœ¼ë¡œ)
            board_selectors = [
                "a[href*='BoardList.nhn']",
                "a[href*='menuid=']",
                ".menu_list a",
                ".board_list a",
                "[class*='menu'] a",
                "[class*='board'] a",
                "a[href*='cafe.naver.com']",
                ".cafe_menu a",
                ".menu_item a"
            ]
            
            board_links = []
            for selector in board_selectors:
                try:
                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if links:
                        board_links = links
                        print(f"âœ… ê²Œì‹œíŒ ë§í¬ {len(links)}ê°œ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not board_links:
                print("âš ï¸ ê²Œì‹œíŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return boards
            
            print(f"ğŸ” ë°œê²¬ëœ ë§í¬ë“¤ì„ ë¶„ì„ ì¤‘...")
            
            for i, link in enumerate(board_links):
                try:
                    href = link.get_attribute("href")
                    text = link.text.strip() if link.text else ""
                    
                    print(f"ë§í¬ {i+1}: {text} -> {href}")
                    
                    # ê²Œì‹œíŒ ë§í¬ì¸ì§€ í™•ì¸ (ë” ìœ ì—°í•œ ì¡°ê±´)
                    if not href or not any(keyword in href.lower() for keyword in ['boardlist', 'menuid', 'cafe.naver.com']):
                        continue
                    
                    # ë©”ë‰´ ID ì¶”ì¶œ (ë” ìœ ì—°í•œ íŒ¨í„´)
                    import re
                    menuid_match = re.search(r'menuid=(\d+)', href)
                    if menuid_match:
                        menu_id = menuid_match.group(1)
                    else:
                        # menuidê°€ ì—†ìœ¼ë©´ hrefì—ì„œ ìˆ«ì ì¶”ì¶œ
                        numbers = re.findall(r'\d+', href)
                        menu_id = numbers[-1] if numbers else f"unknown_{i}"
                    
                    menu_name = text if text else f"ê²Œì‹œíŒ {menu_id}"
                    
                    # URL ì •ê·œí™”
                    if href.startswith("/"):
                        href = f"https://cafe.naver.com{href}"
                    elif not href.startswith("http"):
                        href = f"https://cafe.naver.com/{href}"
                    
                    boards.append({
                        "menu_id": menu_id,
                        "menu_name": menu_name,
                        "board_url": href
                    })
                    
                    print(f"âœ… ê²Œì‹œíŒ ì¶”ê°€: {menu_name} (ID: {menu_id})")
                    
                except Exception as e:
                    print(f"âš ï¸ ê²Œì‹œíŒ ë§í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        print(f"ğŸ“Š ì´ {len(boards)}ê°œ ê²Œì‹œíŒ ì¶”ì¶œ ì™„ë£Œ")
        return boards
    
    def scrape_cafe(self, cafe_url: str, max_pages: int, all_boards: bool, selected_boards: list[str], include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None) -> list[dict]:
        """ì¹´í˜ ì „ì²´ ë˜ëŠ” íŠ¹ì • ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘"""
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ì„ ê°„ì†Œí™” (ì´ë¯¸ ê²Œì‹œíŒ ì¡°íšŒì—ì„œ í™•ì¸ë¨)
        if not self.driver:
            raise Exception("Browser not started")
        
        start_time = time.time()
        all_results = []
        
        # ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
        boards = self.get_cafe_boards(cafe_url)
        
        if not boards:
            raise Exception("ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œíŒ í•„í„°ë§
        if all_boards:
            target_boards = boards
            scraping_logger.log_scraping_start("ì „ì²´ ê²Œì‹œíŒ", len(target_boards))
        else:
            target_boards = [board for board in boards if board["menu_id"] in selected_boards]
            scraping_logger.log_scraping_start("ì„ íƒëœ ê²Œì‹œíŒ", len(target_boards))
        
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ê²Œì‹œíŒ: {len(target_boards)}ê°œ")
        
        # ê° ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
        for i, board in enumerate(target_boards, 1):
            try:
                print(f"ğŸ“„ ê²Œì‹œíŒ {i}/{len(target_boards)}: {board['menu_name']}")
                scraping_logger.log_scraping_progress(i, len(target_boards), board['menu_name'])
                
                # ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
                board_results = self.scrape_board_articles(board["board_url"], max_pages)
                
                # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                article_urls = [article["article_url"] for article in board_results]
                if article_urls:
                    detailed_results = self.scrape_multiple_articles(article_urls, include_nicks, exclude_nicks)
                    all_results.extend(detailed_results)
                
                print(f"âœ… ê²Œì‹œíŒ {i}/{len(target_boards)} ì™„ë£Œ: {len(article_urls)}ê°œ ê²Œì‹œê¸€")
                
                # ê²Œì‹œíŒ ê°„ ì§€ì—°
                if i < len(target_boards):
                    delay = self._calculate_delay(i, len(target_boards))
                    time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œíŒ {i}/{len(target_boards)} ì‹¤íŒ¨: {e}")
                scraping_logger.log_scraping_error(board["menu_name"], str(e))
                continue
        
        # ì™„ë£Œ ë¡œê¹…
        successful = len([r for r in all_results if "error" not in r])
        failed = len(all_results) - successful
        scraping_logger.log_scraping_complete(successful, failed, len(all_results))
        
        return all_results
    
    def batch_scraping(self, cafe_url: str, max_pages: int, all_boards: bool, selected_boards: list[str], search_keywords: list[str], post_authors: list[str], comment_authors: list[str], max_articles: int, image_processing: str, period: str, delay_between_requests: int) -> list[dict]:
        """ë°°ì¹˜ í¬ë¡¤ë§ - í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ì‘ì„±ì í•„í„°ë§ í¬í•¨"""
        try:
            # ë¸Œë¼ìš°ì € ì„¸ì…˜ í™•ì¸ ë° ì¬ì‹œì‘
            if not self.driver:
                print("ğŸ”„ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
                self.start_browser()
            else:
                # ê¸°ì¡´ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ìœ íš¨í•œì§€ í™•ì¸
                try:
                    current_url = self.driver.current_url
                    if not current_url or current_url == "data:,":
                        print("âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠì–´ì§ - ì¬ì‹œì‘ ì¤‘...")
                        self.start_browser()
                except Exception as e:
                    print(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠì–´ì§: {e}")
                    print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
                    self.start_browser()

            # ì¿ í‚¤ ë¡œë“œ
            self._load_cookies()

            # ì¿ í‚¤ê°€ ìˆìœ¼ë©´ ë¡œê·¸ì¸ëœ ê²ƒìœ¼ë¡œ ê°„ì£¼í•˜ê³  ì¹´í˜ ì ‘ì† ì‹œë„
            if self._cookie_file.exists():
                print("âœ… ì €ì¥ëœ ì¿ í‚¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¹´í˜ ì ‘ì†ì„ ì‹œë„í•©ë‹ˆë‹¤.")
            else:
                print("âš ï¸ ì €ì¥ëœ ì¿ í‚¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ’¡ í•´ê²° ë°©ë²•: ì›¹ UIì—ì„œ 'ë¡œê·¸ì¸ ì‹œì‘' ë²„íŠ¼ì„ í´ë¦­í•˜ì„¸ìš”.")
                raise Exception("Login required but failed")
        except Exception as e:
            print(f"âŒ ë°°ì¹˜ í¬ë¡¤ë§ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
        
        start_time = time.time()
        all_results = []
        collected_count = 0
        
        # ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
        boards = self.get_cafe_boards(cafe_url)
        
        if not boards:
            raise Exception("ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œíŒ í•„í„°ë§
        if all_boards:
            target_boards = boards
            scraping_logger.log_scraping_start("ì „ì²´ ê²Œì‹œíŒ ë°°ì¹˜ í¬ë¡¤ë§", len(target_boards))
        else:
            target_boards = [board for board in boards if board["menu_id"] in selected_boards]
            scraping_logger.log_scraping_start("ì„ íƒëœ ê²Œì‹œíŒ ë°°ì¹˜ í¬ë¡¤ë§", len(target_boards))
        
        print(f"ğŸ”„ ë°°ì¹˜ í¬ë¡¤ë§ ëŒ€ìƒ ê²Œì‹œíŒ: {len(target_boards)}ê°œ")
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        print(f"ğŸ‘¤ ê²Œì‹œê¸€ ì‘ì„±ì í•„í„°: {post_authors}")
        print(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„±ì í•„í„°: {comment_authors}")
        
        # ê° ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
        for i, board in enumerate(target_boards, 1):
            if collected_count >= max_articles:
                print(f"ğŸ“Š ìµœëŒ€ ìˆ˜ì§‘ ê²Œì‹œê¸€ ìˆ˜({max_articles})ì— ë„ë‹¬í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
                
            try:
                # ë¸Œë¼ìš°ì € ì„¸ì…˜ í™•ì¸ ë° ì¬ì‹œì‘
                try:
                    current_url = self.driver.current_url
                    # URLì´ ìœ íš¨í•œì§€ í™•ì¸
                    if not current_url or current_url == "data:,":
                        raise Exception("Invalid browser session")
                except Exception as e:
                    print(f"âš ï¸ ë¸Œë¼ìš°ì € ì„¸ì…˜ì´ ëŠì–´ì§: {e}")
                    print("ğŸ”„ ë¸Œë¼ìš°ì € ì¬ì‹œì‘ ì¤‘...")
                    try:
                        self.driver.quit()
                    except:
                        pass
                    self.start_browser()
                    self._load_cookies()
                    
                    # ì¹´í˜ í˜ì´ì§€ë¡œ ë‹¤ì‹œ ì´ë™
                    try:
                        self.driver.get(cafe_url)
                        time.sleep(3)
                    except Exception as nav_e:
                        print(f"âš ï¸ ì¹´í˜ í˜ì´ì§€ ì¬ì ‘ì† ì‹¤íŒ¨: {nav_e}")
                        continue
                
                print(f"ğŸ“„ ê²Œì‹œíŒ {i}/{len(target_boards)}: {board['menu_name']}")
                scraping_logger.log_scraping_progress(i, len(target_boards), board['menu_name'])
                
                # ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
                board_results = self.scrape_board_articles(board["board_url"], max_pages)
                
                # í‚¤ì›Œë“œ ë° ì‘ì„±ì í•„í„°ë§
                filtered_articles = self._filter_articles(
                    board_results, 
                    search_keywords, 
                    post_authors, 
                    comment_authors,
                    period
                )
                
                # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                article_urls = [article["article_url"] for article in filtered_articles]
                if article_urls:
                    # ë‚¨ì€ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ê²Œì‹œê¸€ ìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
                    remaining_articles = max_articles - collected_count
                    if len(article_urls) > remaining_articles:
                        article_urls = article_urls[:remaining_articles]
                    
                    detailed_results = self.scrape_multiple_articles(article_urls, comment_authors, None)
                    all_results.extend(detailed_results)
                    collected_count += len(detailed_results)
                
                print(f"âœ… ê²Œì‹œíŒ {i}/{len(target_boards)} ì™„ë£Œ: {len(article_urls)}ê°œ ê²Œì‹œê¸€ (ëˆ„ì : {collected_count}ê°œ)")
                
                # ê²Œì‹œíŒ ê°„ ì§€ì—°
                if i < len(target_boards):
                    time.sleep(delay_between_requests)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œíŒ {i}/{len(target_boards)} ì‹¤íŒ¨: {e}")
                scraping_logger.log_scraping_error(board["menu_name"], str(e))
                continue
        
        # ì™„ë£Œ ë¡œê¹…
        successful = len([r for r in all_results if "error" not in r])
        failed = len(all_results) - successful
        scraping_logger.log_scraping_complete(successful, failed, len(all_results))
        
        return all_results
    
    def _filter_articles(self, articles: list[dict], search_keywords: list[str], post_authors: list[str], comment_authors: list[str], period: str) -> list[dict]:
        """ê²Œì‹œê¸€ í•„í„°ë§ - í‚¤ì›Œë“œ, ì‘ì„±ì, ê¸°ê°„"""
        filtered = []
        
        print(f"ğŸ” í•„í„°ë§ ì‹œì‘: {len(articles)}ê°œ ê²Œì‹œê¸€ ì¤‘ì—ì„œ")
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        print(f"ğŸ” ì‘ì„±ì í•„í„°: {post_authors}")
        
        for i, article in enumerate(articles):
            title = article.get('title', '').lower()
            author = article.get('author_nickname', '').lower()
            
            # í‚¤ì›Œë“œ í•„í„°ë§
            if search_keywords:
                keyword_match = any(keyword.lower() in title for keyword in search_keywords)
                if not keyword_match:
                    print(f"âŒ í‚¤ì›Œë“œ ë¶ˆì¼ì¹˜: {article.get('title', 'N/A')[:30]}...")
                    continue
                else:
                    print(f"âœ… í‚¤ì›Œë“œ ì¼ì¹˜: {article.get('title', 'N/A')[:30]}...")
            
            # ì‘ì„±ì í•„í„°ë§
            if post_authors:
                author_match = any(author_nick.lower() in author for author_nick in post_authors)
                if not author_match:
                    print(f"âŒ ì‘ì„±ì ë¶ˆì¼ì¹˜: {author}")
                    continue
                else:
                    print(f"âœ… ì‘ì„±ì ì¼ì¹˜: {author}")
            
            # ê¸°ê°„ í•„í„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
            if period != "all":
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‚ ì§œ íŒŒì‹± ë° ë¹„êµ í•„ìš”
                pass
            
            filtered.append(article)
            print(f"âœ… í•„í„° í†µê³¼: {len(filtered)}ë²ˆì§¸ ê²Œì‹œê¸€")
            
            # ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜ ì œí•œ í™•ì¸
            if len(filtered) >= 5:  # 5ê°œë¡œ ì œí•œ
                print(f"ğŸ›‘ ìµœëŒ€ ê²Œì‹œê¸€ ìˆ˜(5ê°œ)ì— ë„ë‹¬í•˜ì—¬ í•„í„°ë§ ì¤‘ë‹¨")
                break
        
        print(f"ğŸ” í•„í„°ë§ ì™„ë£Œ: {len(filtered)}ê°œ ê²Œì‹œê¸€ ì„ íƒ")
        return filtered

    def close(self) -> None:
        """Close browser and save cookies."""
        if self.driver:
            self._save_cookies()
            self.driver.quit()
        print("ğŸ”’ Browser closed, cookies saved.")