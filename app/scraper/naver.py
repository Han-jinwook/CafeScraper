from __future__ import annotations
import asyncio
import json
import os
import time
import psutil
import base64
from pathlib import Path
from typing import Optional
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.common.exceptions import TimeoutException, NoSuchElementException
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import requests

# ë¡œê¹… ì‹œìŠ¤í…œ ì„í¬íŠ¸
try:
    from app.utils.logger import scraping_logger
except ImportError:
    # ë¡œê¹… ì‹œìŠ¤í…œì´ ì—†ì„ ë•Œ ê¸°ë³¸ ë¡œê¹…
    class DummyLogger:
        def info(self, msg): print(f"INFO: {msg}")
        def warning(self, msg): print(f"WARNING: {msg}")
        def error(self, msg): print(f"ERROR: {msg}")
        def debug(self, msg): print(f"DEBUG: {msg}")
        def log_scraping_start(self, url, total): self.info(f"ğŸš€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {url} (ì´ {total}ê°œ)")
        def log_scraping_progress(self, current, total, url): self.info(f"ğŸ“„ [{current:3d}/{total:3d}] {url}")
        def log_scraping_success(self, url, title): self.info(f"âœ… ì„±ê³µ: {title}")
        def log_scraping_error(self, url, error): self.error(f"âŒ ì‹¤íŒ¨: {url} - {error}")
        def log_scraping_complete(self, successful, failed, total): self.info(f"ğŸ“Š ì™„ë£Œ: ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
        def log_performance(self, operation, duration, details=""): self.info(f"â±ï¸ {operation}: {duration:.2f}ì´ˆ")
        def log_memory_usage(self, operation, memory_mb): self.info(f"ğŸ’¾ {operation}: {memory_mb:.1f}MB")
        def log_antibot_measure(self, measure, details=""): self.info(f"ğŸ›¡ï¸ ì•ˆí‹°ë´‡ ëŒ€ì‘: {measure}")
    
    scraping_logger = DummyLogger()

class NaverScraper:
    """Naver Cafe scraper using Selenium WebDriver with manual login and cookie persistence."""

    def __init__(self, sessions_dir: str, snapshots_dir: str) -> None:
        self.sessions_dir = Path(sessions_dir)
        self.snapshots_dir = Path(snapshots_dir)
        self.sessions_dir.mkdir(exist_ok=True)
        self.snapshots_dir.mkdir(exist_ok=True)
        
        self.driver: Optional[webdriver.Chrome] = None
        self._cookie_file = self.sessions_dir / "naver_cookies.json"

    def start_browser(self) -> None:
        """Start Chrome browser with persistent context for cookie management."""
        if self.driver:
            print("âœ… ë¸Œë¼ìš°ì €ê°€ ì´ë¯¸ ì‹¤í–‰ ì¤‘ì…ë‹ˆë‹¤.")
            return
        
        try:
            print("ğŸ”„ Selenium Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
            
            # Chrome ì˜µì…˜ ì„¤ì •
            chrome_options = Options()
            chrome_options.add_argument("--no-sandbox")
            chrome_options.add_argument("--disable-dev-shm-usage")
            chrome_options.add_argument("--disable-blink-features=AutomationControlled")
            chrome_options.add_argument("--disable-gpu")
            chrome_options.add_argument("--disable-extensions")
            chrome_options.add_argument("--disable-plugins")
            chrome_options.add_argument("--disable-images")
            chrome_options.add_argument("--disable-web-security")
            chrome_options.add_argument("--disable-features=VizDisplayCompositor")
            chrome_options.add_argument("--disable-background-timer-throttling")
            chrome_options.add_argument("--disable-backgrounding-occluded-windows")
            chrome_options.add_argument("--disable-renderer-backgrounding")
            chrome_options.add_argument("--disable-field-trial-config")
            chrome_options.add_argument("--disable-back-forward-cache")
            chrome_options.add_argument("--disable-ipc-flooding-protection")
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation", "enable-logging"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            chrome_options.add_experimental_option("prefs", {
                "profile.default_content_setting_values.notifications": 2,
                "profile.default_content_settings.popups": 0,
                "profile.managed_default_content_settings.images": 2
            })
            
            # User-Agent ì„¤ì •
            chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
            
            # í—¤ë“œë¦¬ìŠ¤ ëª¨ë“œ (í•„ìš”ì‹œ ì£¼ì„ í•´ì œ)
            # chrome_options.add_argument("--headless")
            
            # WebDriver ì´ˆê¸°í™”
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            print("âœ… Selenium Chrome ë¸Œë¼ìš°ì € ì‹œì‘ ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {e}")
            print(f"âŒ ì—ëŸ¬ íƒ€ì…: {type(e).__name__}")
            print(f"âŒ ì—ëŸ¬ ìƒì„¸: {str(e)}")
            raise Exception(f"ë¸Œë¼ìš°ì € ì‹œì‘ ì‹¤íŒ¨: {str(e)}")

    def _load_cookies(self) -> None:
        """Load saved cookies from file."""
        if self._cookie_file.exists() and self.driver:
            try:
                with open(self._cookie_file, 'r', encoding='utf-8') as f:
                    cookies = json.load(f)
                
                # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™ í›„ ì¿ í‚¤ ë¡œë“œ
                self.driver.get("https://www.naver.com")
                for cookie in cookies:
                    try:
                        self.driver.add_cookie(cookie)
                    except Exception as e:
                        print(f"âš ï¸ ì¿ í‚¤ ë¡œë“œ ì‹¤íŒ¨: {cookie.get('name', 'unknown')} - {e}")
                        continue
                
                print(f"âœ… Loaded {len(cookies)} cookies from {self._cookie_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to load cookies: {e}")

    def _save_cookies(self) -> None:
        """Save current cookies to file."""
        if self.driver:
            try:
                cookies = self.driver.get_cookies()
                with open(self._cookie_file, 'w', encoding='utf-8') as f:
                    json.dump(cookies, f, ensure_ascii=False, indent=2)
                print(f"âœ… Saved {len(cookies)} cookies to {self._cookie_file}")
            except Exception as e:
                print(f"âš ï¸ Failed to save cookies: {e}")

    def ensure_logged_in(self) -> bool:
        """Ensure user is logged in to Naver. Returns True if successful."""
        if not self.driver:
            self.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        self._load_cookies()
        
        # ë„¤ì´ë²„ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
        self.driver.get("https://www.naver.com")
        time.sleep(2)
        
        # ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
        try:
            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ë¡œê·¸ì¸ëœ ìƒíƒœ
            login_button = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì¸')]")
            if not login_button:
                print("âœ… Already logged in to Naver")
                self._save_cookies()
                return True
            
            # ë¡œê·¸ì¸ ë²„íŠ¼ì´ ìˆìœ¼ë©´ ë¡œê·¸ì¸ í•„ìš”
            print("ğŸ” Manual login required. Please log in to Naver in the browser window.")
            print("   The system will automatically detect when you're logged in...")
            
            # ìë™ ë¡œê·¸ì¸ ê°ì§€ (ìµœëŒ€ 5ë¶„ ëŒ€ê¸°)
            max_wait_time = 300  # 5ë¶„
            check_interval = 3   # 3ì´ˆë§ˆë‹¤ í™•ì¸
            waited_time = 0
            
            while waited_time < max_wait_time:
                time.sleep(check_interval)
                waited_time += check_interval
                
                # í˜ì´ì§€ ìƒˆë¡œê³ ì¹¨ í›„ ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸
                self.driver.refresh()
                time.sleep(2)
                
                # ë¡œê·¸ì¸ ìƒíƒœ ì¬í™•ì¸
                login_button_after = self.driver.find_elements(By.XPATH, "//a[contains(text(), 'ë¡œê·¸ì¸')]")
                if not login_button_after:
                    self._save_cookies()
                    print("âœ… Login successful! Cookies saved.")
                    return True
                
                # ì§„í–‰ ìƒí™© í‘œì‹œ
                remaining_time = max_wait_time - waited_time
                print(f"â³ Waiting for login... ({remaining_time}s remaining)")
            
            print("âŒ Login timeout. Please try again.")
            return False
                
        except Exception as e:
            print(f"âŒ Login check failed: {e}")
            return False

    def scrape_article(self, url: str, include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None, max_retries: int = 3):
        """Scrape a single article with comments and images."""
        if not self.ensure_logged_in():
            raise Exception("Login required but failed")
        
        for attempt in range(max_retries):
            try:
                print(f"ğŸ“„ ìŠ¤í¬ë˜í•‘ ì‹œë„ {attempt + 1}/{max_retries}: {url}")
                
                # Navigate to article with retry logic
                self._navigate_with_retry(url, max_retries=2)
                
                # Take snapshot for debugging
                snapshot_dir = self.snapshots_dir / Path(url).stem
                snapshot_dir.mkdir(exist_ok=True)
                self.driver.save_screenshot(str(snapshot_dir / f"page_attempt_{attempt + 1}.png"))
                
                # Extract article information
                article_data = self._extract_article_data(url)
                
                # Extract images and convert to base64
                images_base64 = self._extract_images()
                
                # Extract comments with filtering
                comments = self._extract_comments(include_nicks, exclude_nicks)
                
                # Combine all data
                result = {
                    **article_data,
                    "images_base64": images_base64,
                    "comments": comments,
                    "scraped_at": time.strftime("%Y-%m-%d %H:%M:%S")
                }
                
                print(f"âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ: {article_data.get('title', 'N/A')}")
                return result
                
            except Exception as e:
                print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5  # 5ì´ˆ, 10ì´ˆ, 15ì´ˆ ëŒ€ê¸°
                    print(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print(f"âŒ ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜ ì´ˆê³¼: {url}")
                    raise e

    def _extract_article_data(self, url: str) -> dict:
        """Extract basic article information."""
        try:
            # Extract cafe ID from URL
            cafe_id = url.split("cafe.naver.com/")[1].split("/")[0] if "cafe.naver.com" in url else "unknown"
            
            # Extract article ID from URL
            article_id = url.split("/")[-1] if "/" in url else "unknown"
            
            # Try multiple selectors for title
            title_selectors = [
                ".title_text",
                ".se-title-text", 
                ".se-fs-",
                "h3.title",
                ".article_title",
                "[data-testid='article-title']",
                ".board_title",
                ".article_title_text",
                ".se-text-paragraph",
                "h1", "h2", "h3",
                ".title",
                "[class*='title']",
                "[class*='Title']"
            ]
            
            title = self._safe_extract(title_selectors, default="ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # Try multiple selectors for author
            author_selectors = [
                ".nick",
                ".nickname", 
                ".author",
                ".writer",
                "[data-testid='author']",
                ".se-fs-",
                ".nickname_text",
                ".author_name",
                ".writer_name",
                "[class*='nick']",
                "[class*='author']",
                "[class*='writer']",
                ".user_info .nick",
                ".user_info .nickname",
                ".article_info .nick",
                ".article_info .nickname"
            ]
            
            author = self._safe_extract(author_selectors, default="ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            
            # Try multiple selectors for content
            content_selectors = [
                ".se-main-container",
                ".se-component-content", 
                ".article_content",
                ".content",
                "[data-testid='article-content']",
                ".se-text-paragraph",
                ".article_text",
                ".board_text",
                ".post_content",
                ".article_body",
                "[class*='content']",
                "[class*='Content']",
                "[class*='text']",
                "[class*='Text']",
                ".se-text",
                ".se-component"
            ]
            
            content_text = self._safe_extract(content_selectors, default="ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            content_html = self._safe_extract_html(content_selectors, default="<p>ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ</p>")
            
            # Try to extract date
            date_selectors = [
                ".date",
                ".time", 
                ".created_at",
                "[data-testid='created-date']",
                ".article_date",
                ".post_date",
                ".board_date",
                "[class*='date']",
                "[class*='Date']",
                "[class*='time']",
                "[class*='Time']",
                ".article_info .date",
                ".article_info .time",
                ".user_info .date",
                ".user_info .time"
            ]
            
            posted_at = self._safe_extract(date_selectors, default=None)
            if posted_at == "ì•Œ ìˆ˜ ì—†ìŒ":
                posted_at = None
            
            return {
                "cafe_id": cafe_id,
                "article_id": article_id,
                "article_url": url,
                "title": title,
                "author_nickname": author,
                "posted_at": posted_at,
                "content_text": content_text,
                "content_html": content_html
            }
            
        except Exception as e:
            print(f"âš ï¸ Error extracting article data: {e}")
            return {
                "cafe_id": "unknown",
                "article_id": "unknown",
                "article_url": url,
                "title": "ì˜¤ë¥˜ ë°œìƒ",
                "author_nickname": "ì•Œ ìˆ˜ ì—†ìŒ",
                "posted_at": None,
                "content_text": f"ì˜¤ë¥˜: {str(e)}",
                "content_html": f"<p>ì˜¤ë¥˜: {str(e)}</p>"
            }

    def _extract_images(self, max_images: int = 10, max_size_mb: float = 5.0) -> list:
        """Extract images and convert to base64 with memory optimization."""
        images = []
        try:
            # Find all images in the article
            image_elements = self.driver.find_elements(By.TAG_NAME, "img")
            
            # Limit number of images to prevent memory issues
            if len(image_elements) > max_images:
                print(f"âš ï¸ Too many images ({len(image_elements)}), limiting to {max_images}")
                image_elements = image_elements[:max_images]
            
            for i, img in enumerate(image_elements):
                try:
                    # Get image source
                    src = img.get_attribute("src")
                    if not src or src.startswith("data:"):
                        continue
                    
                    # Download image and convert to base64
                    response = requests.get(src, timeout=10)
                    if response.status_code == 200:
                        image_data = response.content
                        
                        # Check image size to prevent memory issues
                        size_mb = len(image_data) / (1024 * 1024)
                        if size_mb > max_size_mb:
                            print(f"âš ï¸ Image {i+1} too large ({size_mb:.1f}MB), skipping")
                            continue
                        
                        base64_data = base64.b64encode(image_data).decode('utf-8')
                        
                        # Determine MIME type
                        mime_type = "image/jpeg"
                        if src.lower().endswith('.png'):
                            mime_type = "image/png"
                        elif src.lower().endswith('.gif'):
                            mime_type = "image/gif"
                        
                        images.append({
                            "mime": mime_type,
                            "data": base64_data,
                            "filename": f"image_{i+1}.jpg",
                            "size_mb": round(size_mb, 2)
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ Error processing image {i}: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting images: {e}")
        
        return images

    def _extract_comments(self, include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None) -> list:
        """Extract comments with nickname filtering."""
        comments = []
        try:
            # Try multiple selectors for comments
            comment_selectors = [
                ".comment",
                ".reply", 
                ".comment_item",
                "[data-testid='comment']",
                ".comment_list .comment",
                ".comment_list .reply",
                ".reply_list .comment",
                ".reply_list .reply",
                "[class*='comment']",
                "[class*='Comment']",
                "[class*='reply']",
                "[class*='Reply']",
                ".comment_area .comment",
                ".comment_area .reply",
                ".reply_area .comment",
                ".reply_area .reply"
            ]
            
            comment_elements = []
            for selector in comment_selectors:
                try:
                    elements = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if elements:
                        comment_elements = elements
                        break
                except:
                    continue
            
            for comment_element in comment_elements:
                try:
                    # Extract comment text
                    text_element = comment_element.find_elements(By.CSS_SELECTOR, ".comment_text, .reply_text, .content")
                    text = text_element[0].text if text_element else "ëŒ“ê¸€ ë‚´ìš© ì—†ìŒ"
                    
                    # Extract author
                    author_element = comment_element.find_elements(By.CSS_SELECTOR, ".nick, .nickname, .author")
                    author = author_element[0].text if author_element else "ì•Œ ìˆ˜ ì—†ìŒ"
                    
                    # Extract date
                    date_element = comment_element.find_elements(By.CSS_SELECTOR, ".date, .time")
                    date = date_element[0].text if date_element else None
                    
                    # Apply nickname filtering
                    should_include = True
                    
                    if include_nicks and include_nicks:
                        should_include = any(nick in author for nick in include_nicks)
                    
                    if exclude_nicks and exclude_nicks:
                        should_include = should_include and not any(nick in author for nick in exclude_nicks)
                    
                    if should_include:
                        comments.append({
                            "comment_id": f"comment_{len(comments)+1}",
                            "nickname": author.strip() if author else "ì•Œ ìˆ˜ ì—†ìŒ",
                            "text": text.strip() if text else "ëŒ“ê¸€ ë‚´ìš© ì—†ìŒ",
                            "created_at": date.strip() if date else None
                        })
                        
                except Exception as e:
                    print(f"âš ï¸ Error processing comment: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting comments: {e}")
        
        return comments

    def scrape_board_articles(self, board_url: str, max_pages: int = 5) -> list[dict]:
        """Scrape articles from a board page with pagination."""
        if not self.ensure_logged_in():
            raise Exception("Login required but failed")
        
        articles = []
        page = 1
        total_articles = 0
        
        print(f"ğŸ“Š ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {board_url}")
        print(f"ğŸ“„ ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print("=" * 50)
        
        while page <= max_pages:
            try:
                progress = f"[í˜ì´ì§€ {page:2d}/{max_pages:2d}]"
                print(f"ğŸ“„ {progress} ê²Œì‹œíŒ í˜ì´ì§€ ë¡œë”© ì¤‘...")
                
                # Navigate to board page
                page_url = f"{board_url}?page={page}" if "?" not in board_url else f"{board_url}&page={page}"
                self.driver.get(page_url)
                time.sleep(2)
                
                # Take snapshot for debugging
                snapshot_dir = self.snapshots_dir / f"board_page_{page}"
                snapshot_dir.mkdir(exist_ok=True)
                self.driver.save_screenshot(str(snapshot_dir / "page.png"))
                
                # Extract article links from current page
                page_articles = self._extract_article_links_from_board()
                
                if not page_articles:
                    print(f"ğŸ“„ {progress} ê²Œì‹œê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ, í˜ì´ì§€ë„¤ì´ì…˜ ì¤‘ë‹¨")
                    break
                
                articles.extend(page_articles)
                total_articles += len(page_articles)
                print(f"âœ… {progress} ì™„ë£Œ - ë°œê²¬ëœ ê²Œì‹œê¸€: {len(page_articles)}ê°œ (ëˆ„ì : {total_articles}ê°œ)")
                
                page += 1
                
                # Add delay to avoid being blocked
                time.sleep(2)
                
            except Exception as e:
                print(f"âš ï¸ {progress} ì˜¤ë¥˜: {e}")
                break
        
        print("=" * 50)
        print(f"ğŸ“Š ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {total_articles}ê°œ ê²Œì‹œê¸€ ë°œê²¬")
        return articles

    def _extract_article_links_from_board(self) -> list[dict]:
        """Extract article links and basic info from board page."""
        articles = []
        
        try:
            # Try multiple selectors for article links
            article_selectors = [
                "a[href*='/ArticleRead.nhn']",
                "a[href*='/ArticleRead']", 
                ".article",
                ".board_list a",
                ".list_item a",
                "tr td a"
            ]
            
            article_links = []
            for selector in article_selectors:
                try:
                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if links:
                        article_links = links
                        print(f"âœ… Found {len(links)} article links using selector: {selector}")
                        break
                except:
                    continue
            
            if not article_links:
                print("âš ï¸ No article links found on this page")
                return articles
            
            for link in article_links:
                try:
                    # Get article URL
                    href = link.get_attribute("href")
                    if not href or "ArticleRead" not in href:
                        continue
                    
                    # Make URL absolute
                    if href.startswith("/"):
                        href = f"https://cafe.naver.com{href}"
                    elif not href.startswith("http"):
                        href = f"https://cafe.naver.com/{href}"
                    
                    # Extract basic info from link text
                    title = link.text.strip() if link.text else "ì œëª© ì—†ìŒ"
                    
                    # Try to find author and date from parent elements
                    parent_row = link.find_element(By.XPATH, "ancestor::tr")
                    author = "ì•Œ ìˆ˜ ì—†ìŒ"
                    date = None
                    
                    if parent_row:
                        # Try to find author in the same row
                        author_selectors = [".nick", ".nickname", ".author", "td:nth-child(2)", "td:nth-child(3)"]
                        for author_sel in author_selectors:
                            try:
                                author_elem = parent_row.find_elements(By.CSS_SELECTOR, author_sel)
                                if author_elem:
                                    author_text = author_elem[0].text
                                    if author_text and author_text.strip():
                                        author = author_text.strip()
                                        break
                            except:
                                continue
                        
                        # Try to find date in the same row
                        date_selectors = [".date", ".time", "td:last-child", "td:nth-last-child(2)"]
                        for date_sel in date_selectors:
                            try:
                                date_elem = parent_row.find_elements(By.CSS_SELECTOR, date_sel)
                                if date_elem:
                                    date_text = date_elem[0].text
                                    if date_text and date_text.strip():
                                        date = date_text.strip()
                                        break
                            except:
                                continue
                    
                    # Extract article ID from URL
                    article_id = href.split("/")[-1] if "/" in href else "unknown"
                    
                    articles.append({
                        "article_id": article_id,
                        "article_url": href,
                        "title": title,
                        "author_nickname": author,
                        "posted_at": date,
                        "scraped_at": None  # Will be filled when scraping full article
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ Error processing article link: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ Error extracting article links: {e}")
        
        return articles

    def scrape_multiple_articles(self, article_urls: list[str], include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None, max_concurrent: int = 3) -> list[dict]:
        """Scrape multiple articles with progress tracking."""
        if not self.ensure_logged_in():
            raise Exception("Login required but failed")
        
        start_time = time.time()
        results = []
        total = len(article_urls)
        successful = 0
        failed = 0
        
        # ë¡œê¹… ì‹œì‘
        scraping_logger.log_scraping_start("ë‹¤ì¤‘ ê²Œì‹œê¸€", total)
        scraping_logger.log_performance("ë™ì‹œ ì²˜ë¦¬ ì„¤ì •", 0, f"ìµœëŒ€ {max_concurrent}ê°œ")
        
        print(f"ğŸš€ ë‹¤ì¤‘ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {total}ê°œ ê²Œì‹œê¸€")
        print("=" * 60)
        
        # Process articles sequentially (Selenium doesn't support true concurrency)
        for i, url in enumerate(article_urls, 1):
            try:
                progress = f"[{i:3d}/{total:3d}]"
                percentage = (i / total) * 100
                print(f"ğŸ“„ {progress} ({percentage:5.1f}%) ìŠ¤í¬ë˜í•‘ ì¤‘: {url}")
                
                # Scrape individual article
                article_data = self.scrape_article(url, include_nicks, exclude_nicks)
                results.append(article_data)
                successful += 1
                print(f"âœ… {progress} ì™„ë£Œ")
                
                # Add delay between requests
                if i < total:
                    delay = self._calculate_delay(i, total)
                    scraping_logger.log_antibot_measure("ìš”ì²­ ê°„ ëŒ€ê¸°", f"{delay}ì´ˆ")
                    time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ {progress} ì‹¤íŒ¨: {e}")
                failed += 1
                results.append({
                    "article_url": url,
                    "title": "ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨",
                    "error": str(e),
                    "scraped_at": None
                })
        
        print("=" * 60)
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: ì´ {total}ê°œ ì¤‘ ì„±ê³µ {successful}ê°œ, ì‹¤íŒ¨ {failed}ê°œ")
        return results

    def _navigate_with_retry(self, url: str, max_retries: int = 3) -> None:
        """Navigate to URL with retry logic."""
        for attempt in range(max_retries):
            try:
                self.driver.get(url)
                time.sleep(3)  # Wait for page to load
                return
            except Exception as e:
                print(f"âš ï¸ ë„¤ë¹„ê²Œì´ì…˜ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    time.sleep(2)
                else:
                    raise e

    def _safe_extract(self, selectors: list[str], timeout: int = 2, default: str = "ì•Œ ìˆ˜ ì—†ìŒ") -> str:
        """Safely extract text using multiple selectors."""
        for selector in selectors:
            try:
                element = WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                if element and element.text.strip():
                    return element.text.strip()
            except:
                continue
        return default

    def _safe_extract_html(self, selectors: list[str], timeout: int = 2, default: str = "<p>ì•Œ ìˆ˜ ì—†ìŒ</p>") -> str:
        """Safely extract HTML using multiple selectors."""
        for selector in selectors:
            try:
                element = WebDriverWait(self.driver, timeout).until(
                    EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                )
                if element and element.get_attribute("innerHTML").strip():
                    return element.get_attribute("innerHTML").strip()
            except:
                continue
        return default

    def _calculate_delay(self, current: int, total: int) -> float:
        """ë™ì  ì§€ì—° ì‹œê°„ ê³„ì‚° - ì•ˆí‹°ë´‡ ëŒ€ì‘"""
        import random
        
        # ê¸°ë³¸ ì§€ì—° ì‹œê°„ (2-5ì´ˆ)
        base_delay = random.uniform(2.0, 5.0)
        
        # ì§„í–‰ë¥ ì— ë”°ë¥¸ ì¶”ê°€ ì§€ì—° (í›„ë°˜ë¶€ì¼ìˆ˜ë¡ ë” ì˜¤ë˜ ëŒ€ê¸°)
        progress_factor = current / total
        additional_delay = progress_factor * 3.0  # ìµœëŒ€ 3ì´ˆ ì¶”ê°€
        
        # ëœë¤ ìš”ì†Œ ì¶”ê°€
        random_factor = random.uniform(0.5, 1.5)
        
        total_delay = (base_delay + additional_delay) * random_factor
        
        # ìµœëŒ€ 10ì´ˆë¡œ ì œí•œ
        return min(total_delay, 10.0)
    
    def _get_memory_usage(self) -> float:
        """í˜„ì¬ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ ë°˜í™˜ (MB)"""
        try:
            process = psutil.Process()
            memory_info = process.memory_info()
            return memory_info.rss / (1024 * 1024)  # MB ë‹¨ìœ„
        except:
            return 0.0

    def get_cafe_boards(self, cafe_url: str) -> list[dict]:
        """ì¹´í˜ì˜ ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ"""
        try:
            # ë¸Œë¼ìš°ì €ê°€ ì‹œì‘ë˜ì§€ ì•Šì•˜ìœ¼ë©´ ì‹œì‘
            if not self.driver:
                print("ğŸ”„ ë¸Œë¼ìš°ì € ì´ˆê¸°í™” ì¤‘...")
                self.start_browser()
            
            # ë¡œê·¸ì¸ í™•ì¸
            if not self.ensure_logged_in():
                raise Exception("Login required but failed")
            
            # ì¹´í˜ ë©”ì¸ í˜ì´ì§€ë¡œ ì´ë™
            print(f"ğŸŒ ì¹´í˜ í˜ì´ì§€ ì´ë™: {cafe_url}")
            self.driver.get(cafe_url)
            time.sleep(3)
            
            # ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ
            boards = self._extract_cafe_boards()
            
            scraping_logger.log_scraping_success(cafe_url, f"ê²Œì‹œíŒ {len(boards)}ê°œ ì¡°íšŒ")
            return boards
            
        except Exception as e:
            scraping_logger.log_scraping_error(cafe_url, str(e))
            raise e
    
    def _extract_cafe_boards(self) -> list[dict]:
        """ì¹´í˜ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ"""
        boards = []
        
        try:
            # ê²Œì‹œíŒ ë§í¬ ì„ íƒìë“¤
            board_selectors = [
                "a[href*='BoardList.nhn']",
                "a[href*='menuid=']",
                ".menu_list a",
                ".board_list a",
                "[class*='menu'] a",
                "[class*='board'] a"
            ]
            
            board_links = []
            for selector in board_selectors:
                try:
                    links = self.driver.find_elements(By.CSS_SELECTOR, selector)
                    if links:
                        board_links = links
                        print(f"âœ… ê²Œì‹œíŒ ë§í¬ {len(links)}ê°œ ë°œê²¬: {selector}")
                        break
                except:
                    continue
            
            if not board_links:
                print("âš ï¸ ê²Œì‹œíŒ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                return boards
            
            for link in board_links:
                try:
                    href = link.get_attribute("href")
                    if not href or "BoardList" not in href:
                        continue
                    
                    # ë©”ë‰´ ID ì¶”ì¶œ
                    import re
                    menuid_match = re.search(r'menuid=(\d+)', href)
                    if not menuid_match:
                        continue
                    
                    menu_id = menuid_match.group(1)
                    menu_name = link.text.strip() if link.text else f"ê²Œì‹œíŒ {menu_id}"
                    
                    # URL ì •ê·œí™”
                    if href.startswith("/"):
                        href = f"https://cafe.naver.com{href}"
                    elif not href.startswith("http"):
                        href = f"https://cafe.naver.com/{href}"
                    
                    boards.append({
                        "menu_id": menu_id,
                        "menu_name": menu_name,
                        "board_url": href
                    })
                    
                except Exception as e:
                    print(f"âš ï¸ ê²Œì‹œíŒ ë§í¬ ì²˜ë¦¬ ì˜¤ë¥˜: {e}")
                    continue
                    
        except Exception as e:
            print(f"âš ï¸ ê²Œì‹œíŒ ëª©ë¡ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
        
        return boards
    
    def scrape_cafe(self, cafe_url: str, max_pages: int, all_boards: bool, selected_boards: list[str], include_nicks: list[str] | None = None, exclude_nicks: list[str] | None = None) -> list[dict]:
        """ì¹´í˜ ì „ì²´ ë˜ëŠ” íŠ¹ì • ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘"""
        if not self.ensure_logged_in():
            raise Exception("Login required but failed")
        
        start_time = time.time()
        all_results = []
        
        # ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
        boards = self.get_cafe_boards(cafe_url)
        
        if not boards:
            raise Exception("ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œíŒ í•„í„°ë§
        if all_boards:
            target_boards = boards
            scraping_logger.log_scraping_start("ì „ì²´ ê²Œì‹œíŒ", len(target_boards))
        else:
            target_boards = [board for board in boards if board["menu_id"] in selected_boards]
            scraping_logger.log_scraping_start("ì„ íƒëœ ê²Œì‹œíŒ", len(target_boards))
        
        print(f"ğŸ“Š ìŠ¤í¬ë˜í•‘ ëŒ€ìƒ ê²Œì‹œíŒ: {len(target_boards)}ê°œ")
        
        # ê° ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
        for i, board in enumerate(target_boards, 1):
            try:
                print(f"ğŸ“„ ê²Œì‹œíŒ {i}/{len(target_boards)}: {board['menu_name']}")
                scraping_logger.log_scraping_progress(i, len(target_boards), board['menu_name'])
                
                # ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
                board_results = self.scrape_board_articles(board["board_url"], max_pages)
                
                # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                article_urls = [article["article_url"] for article in board_results]
                if article_urls:
                    detailed_results = self.scrape_multiple_articles(article_urls, include_nicks, exclude_nicks)
                    all_results.extend(detailed_results)
                
                print(f"âœ… ê²Œì‹œíŒ {i}/{len(target_boards)} ì™„ë£Œ: {len(article_urls)}ê°œ ê²Œì‹œê¸€")
                
                # ê²Œì‹œíŒ ê°„ ì§€ì—°
                if i < len(target_boards):
                    delay = self._calculate_delay(i, len(target_boards))
                    time.sleep(delay)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œíŒ {i}/{len(target_boards)} ì‹¤íŒ¨: {e}")
                scraping_logger.log_scraping_error(board["menu_name"], str(e))
                continue
        
        # ì™„ë£Œ ë¡œê¹…
        successful = len([r for r in all_results if "error" not in r])
        failed = len(all_results) - successful
        scraping_logger.log_scraping_complete(successful, failed, len(all_results))
        
        return all_results
    
    def batch_scraping(self, cafe_url: str, max_pages: int, all_boards: bool, selected_boards: list[str], search_keywords: list[str], post_authors: list[str], comment_authors: list[str], max_articles: int, image_processing: str, period: str, delay_between_requests: int) -> list[dict]:
        """ë°°ì¹˜ í¬ë¡¤ë§ - í‚¤ì›Œë“œ ê²€ìƒ‰ ë° ì‘ì„±ì í•„í„°ë§ í¬í•¨"""
        if not self.ensure_logged_in():
            raise Exception("Login required but failed")
        
        start_time = time.time()
        all_results = []
        collected_count = 0
        
        # ê²Œì‹œíŒ ëª©ë¡ ì¡°íšŒ
        boards = self.get_cafe_boards(cafe_url)
        
        if not boards:
            raise Exception("ê²Œì‹œíŒì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
        
        # ìŠ¤í¬ë˜í•‘í•  ê²Œì‹œíŒ í•„í„°ë§
        if all_boards:
            target_boards = boards
            scraping_logger.log_scraping_start("ì „ì²´ ê²Œì‹œíŒ ë°°ì¹˜ í¬ë¡¤ë§", len(target_boards))
        else:
            target_boards = [board for board in boards if board["menu_id"] in selected_boards]
            scraping_logger.log_scraping_start("ì„ íƒëœ ê²Œì‹œíŒ ë°°ì¹˜ í¬ë¡¤ë§", len(target_boards))
        
        print(f"ğŸ”„ ë°°ì¹˜ í¬ë¡¤ë§ ëŒ€ìƒ ê²Œì‹œíŒ: {len(target_boards)}ê°œ")
        print(f"ğŸ” ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        print(f"ğŸ‘¤ ê²Œì‹œê¸€ ì‘ì„±ì í•„í„°: {post_authors}")
        print(f"ğŸ’¬ ëŒ“ê¸€ ì‘ì„±ì í•„í„°: {comment_authors}")
        
        # ê° ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
        for i, board in enumerate(target_boards, 1):
            if collected_count >= max_articles:
                print(f"ğŸ“Š ìµœëŒ€ ìˆ˜ì§‘ ê²Œì‹œê¸€ ìˆ˜({max_articles})ì— ë„ë‹¬í•˜ì—¬ í¬ë¡¤ë§ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                break
                
            try:
                print(f"ğŸ“„ ê²Œì‹œíŒ {i}/{len(target_boards)}: {board['menu_name']}")
                scraping_logger.log_scraping_progress(i, len(target_boards), board['menu_name'])
                
                # ê²Œì‹œíŒ ìŠ¤í¬ë˜í•‘
                board_results = self.scrape_board_articles(board["board_url"], max_pages)
                
                # í‚¤ì›Œë“œ ë° ì‘ì„±ì í•„í„°ë§
                filtered_articles = self._filter_articles(
                    board_results, 
                    search_keywords, 
                    post_authors, 
                    comment_authors,
                    period
                )
                
                # ê° ê²Œì‹œê¸€ ìƒì„¸ ìŠ¤í¬ë˜í•‘
                article_urls = [article["article_url"] for article in filtered_articles]
                if article_urls:
                    # ë‚¨ì€ ìˆ˜ì§‘ ê°€ëŠ¥í•œ ê²Œì‹œê¸€ ìˆ˜ë§Œí¼ë§Œ ì²˜ë¦¬
                    remaining_articles = max_articles - collected_count
                    if len(article_urls) > remaining_articles:
                        article_urls = article_urls[:remaining_articles]
                    
                    detailed_results = self.scrape_multiple_articles(article_urls, comment_authors, None)
                    all_results.extend(detailed_results)
                    collected_count += len(detailed_results)
                
                print(f"âœ… ê²Œì‹œíŒ {i}/{len(target_boards)} ì™„ë£Œ: {len(article_urls)}ê°œ ê²Œì‹œê¸€ (ëˆ„ì : {collected_count}ê°œ)")
                
                # ê²Œì‹œíŒ ê°„ ì§€ì—°
                if i < len(target_boards):
                    time.sleep(delay_between_requests)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œíŒ {i}/{len(target_boards)} ì‹¤íŒ¨: {e}")
                scraping_logger.log_scraping_error(board["menu_name"], str(e))
                continue
        
        # ì™„ë£Œ ë¡œê¹…
        successful = len([r for r in all_results if "error" not in r])
        failed = len(all_results) - successful
        scraping_logger.log_scraping_complete(successful, failed, len(all_results))
        
        return all_results
    
    def _filter_articles(self, articles: list[dict], search_keywords: list[str], post_authors: list[str], comment_authors: list[str], period: str) -> list[dict]:
        """ê²Œì‹œê¸€ í•„í„°ë§ - í‚¤ì›Œë“œ, ì‘ì„±ì, ê¸°ê°„"""
        filtered = []
        
        for article in articles:
            # í‚¤ì›Œë“œ í•„í„°ë§
            if search_keywords:
                title = article.get('title', '').lower()
                content = article.get('content_text', '').lower()
                
                keyword_match = any(keyword.lower() in title or keyword.lower() in content 
                                  for keyword in search_keywords)
                if not keyword_match:
                    continue
            
            # ì‘ì„±ì í•„í„°ë§
            if post_authors:
                author = article.get('author_nickname', '').lower()
                author_match = any(author_nick.lower() in author for author_nick in post_authors)
                if not author_match:
                    continue
            
            # ê¸°ê°„ í•„í„°ë§ (ê°„ë‹¨í•œ êµ¬í˜„)
            if period != "all":
                # ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ë‚ ì§œ íŒŒì‹± ë° ë¹„êµ í•„ìš”
                pass
            
            filtered.append(article)
        
        return filtered

    def close(self) -> None:
        """Close browser and save cookies."""
        if self.driver:
            self._save_cookies()
            self.driver.quit()
        print("ğŸ”’ Browser closed, cookies saved.")