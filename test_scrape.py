#!/usr/bin/env python3
"""
ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ìŠ¤í¬ë¦½íŠ¸
ì‚¬ìš©ë²•: python test_scrape.py
"""
import asyncio
import sys
from pathlib import Path

# Add app directory to path
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper
from app.utils.csv_writer import append_article_bundle_row

async def test_scrape_article():
    """í…ŒìŠ¤íŠ¸ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘"""
    print("ğŸš€ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL ì…ë ¥
    test_url = input("í…ŒìŠ¤íŠ¸í•  ë„¤ì´ë²„ ì¹´í˜ ê²Œì‹œê¸€ URLì„ ì…ë ¥í•˜ì„¸ìš”: ").strip()
    if not test_url:
        print("âŒ URLì´ ì…ë ¥ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    # ì¹´í˜ ID ì¶”ì¶œ (URLì—ì„œ)
    try:
        cafe_id = test_url.split("cafe.naver.com/")[1].split("/")[0]
        print(f"ğŸ“ ì¹´í˜ ID: {cafe_id}")
    except:
        cafe_id = "unknown"
        print("âš ï¸ ì¹´í˜ IDë¥¼ ìë™ìœ¼ë¡œ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
    
    # ëŒ“ê¸€ í•„í„° ì„¤ì •
    include_nicks = input("í¬í•¨í•  ë‹‰ë„¤ì„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì—”í„°ì‹œ ì „ì²´): ").strip()
    include_list = [nick.strip() for nick in include_nicks.split(",")] if include_nicks else None
    
    exclude_nicks = input("ì œì™¸í•  ë‹‰ë„¤ì„ (ì‰¼í‘œë¡œ êµ¬ë¶„, ì—”í„°ì‹œ ì—†ìŒ): ").strip()
    exclude_list = [nick.strip() for nick in exclude_nicks.split(",")] if exclude_nicks else None
    
    print(f"ğŸ” ëŒ“ê¸€ í•„í„° - í¬í•¨: {include_list}, ì œì™¸: {exclude_list}")
    
    # ë””ë ‰í„°ë¦¬ ì„¤ì •
    sessions_dir = Path("sessions")
    snapshots_dir = Path("snapshots")
    outputs_dir = Path("outputs")
    
    scraper = NaverScraper(str(sessions_dir), str(snapshots_dir))
    
    try:
        # ë¡œê·¸ì¸ í™•ì¸
        print("ğŸ” ë¡œê·¸ì¸ ìƒíƒœ í™•ì¸ ì¤‘...")
        if not await scraper.ensure_logged_in():
            print("âŒ ë¡œê·¸ì¸ì´ í•„ìš”í•©ë‹ˆë‹¤. ë¨¼ì € python test_login.pyë¥¼ ì‹¤í–‰í•˜ì„¸ìš”.")
            return
        
        # ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘
        print(f"ğŸ“„ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ ì¤‘: {test_url}")
        
        # TODO: ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ë¡œì§ êµ¬í˜„
        # ì‹¤ì œ ìŠ¤í¬ë˜í•‘ ìˆ˜í–‰
        result = await scraper.scrape_article(
            test_url,
            include_list,
            exclude_list
        )
        
        # CSV ì €ì¥
        csv_path = append_article_bundle_row(outputs_dir, result)
        print(f"âœ… CSV ì €ì¥ ì™„ë£Œ: {csv_path}")
        
        # ê²°ê³¼ í™•ì¸
        print("\nğŸ“Š ìŠ¤í¬ë˜í•‘ ê²°ê³¼:")
        print(f"  - ì¹´í˜ ID: {result['cafe_id']}")
        print(f"  - ì œëª©: {result['title']}")
        print(f"  - ì‘ì„±ì: {result['author_nickname']}")
        print(f"  - ì´ë¯¸ì§€ ìˆ˜: {len(result['images_base64'])}")
        print(f"  - ëŒ“ê¸€ ìˆ˜: {len(result['comments'])}")
        
    except Exception as e:
        print(f"âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
    finally:
        await scraper.close()
        print("ğŸ”’ ë¸Œë¼ìš°ì €ê°€ ì¢…ë£Œë˜ì—ˆìŠµë‹ˆë‹¤.")

if __name__ == "__main__":
    asyncio.run(test_scrape_article())
