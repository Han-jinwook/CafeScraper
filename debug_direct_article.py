#!/usr/bin/env python3
"""
ì§ì ‘ ê²Œì‹œê¸€ URLë¡œ ì ‘ê·¼í•˜ì—¬ HTML êµ¬ì¡° ë¶„ì„
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def analyze_direct_article():
    """ì§ì ‘ ê²Œì‹œê¸€ URLë¡œ ì ‘ê·¼í•˜ì—¬ ë¶„ì„"""
    
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # ì¤‘ê³ ë‚˜ë¼ ì¹´í˜ì˜ ì‹¤ì œ ê²Œì‹œê¸€ URLë“¤ ì‹œë„
        test_urls = [
            "https://cafe.naver.com/joonggonara/1",  # ì¤‘ê³ ë‚˜ë¼ ê²Œì‹œíŒ
            "https://cafe.naver.com/steamindiegame/1",  # ìŠ¤íŒ€ ì¸ë””ê²Œì„ ê²Œì‹œíŒ
            "https://cafe.naver.com/steamindiegame/2",  # ìŠ¤íŒ€ ì¸ë””ê²Œì„ ê²Œì‹œíŒ 2í˜ì´ì§€
        ]
        
        for url in test_urls:
            print(f"\nğŸŒ ê²Œì‹œíŒ ì ‘ì† ì‹œë„: {url}")
            try:
                scraper.driver.get(url)
                time.sleep(5)  # ê¸°ë³¸ ë¡œë”© ëŒ€ê¸°
                
                # JavaScript ë¡œë”© ëŒ€ê¸°
                print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
                time.sleep(30)
                
                # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
                print("ğŸ” ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°...")
                article_selectors = [
                    "a[href*='ArticleRead']",
                    "a[href*='ArticleRead.nhn']",
                    ".article a",
                    ".board_list a",
                    "tr td a",
                    "a[href*='cafe.naver.com']"
                ]
                
                article_links = []
                for selector in article_selectors:
                    try:
                        links = scraper.driver.find_elements("css selector", selector)
                        if links:
                            article_links = links
                            print(f"   {selector}: {len(links)}ê°œ ë°œê²¬")
                            break
                    except:
                        continue
                
                if article_links:
                    print(f"âœ… ê²Œì‹œê¸€ ë§í¬ {len(article_links)}ê°œ ë°œê²¬!")
                    
                    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë§í¬ í´ë¦­
                    first_link = article_links[0]
                    article_url = first_link.get_attribute("href")
                    print(f"ğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ì ‘ì†: {article_url}")
                    
                    scraper.driver.get(article_url)
                    time.sleep(5)  # ê¸°ë³¸ ë¡œë”© ëŒ€ê¸°
                    
                    # JavaScript ë¡œë”© ëŒ€ê¸° (ë” ê¸´ ì‹œê°„)
                    print("â³ ê²Œì‹œê¸€ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
                    time.sleep(30)
                    
                    # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                    scraper.driver.save_screenshot("debug_direct_article.png")
                    print("ğŸ“¸ ìŠ¤í¬ë¦°ìƒ· ì €ì¥: debug_direct_article.png")
                    
                    # HTML ì €ì¥
                    page_source = scraper.driver.page_source
                    with open("debug_direct_article.html", "w", encoding="utf-8") as f:
                        f.write(page_source)
                    print("ğŸ’¾ HTML ì €ì¥: debug_direct_article.html")
                    
                    # ì œëª© ë¶„ì„
                    print("\nğŸ” ì œëª© ë¶„ì„:")
                    h_elements = scraper.driver.find_elements("css selector", "h1, h2, h3, h4, h5, h6")
                    print(f"   h íƒœê·¸: {len(h_elements)}ê°œ")
                    
                    for i, elem in enumerate(h_elements[:10]):
                        try:
                            text = elem.text.strip()
                            tag_name = elem.tag_name
                            class_name = elem.get_attribute("class") or ""
                            if text and len(text) > 5:
                                print(f"   {i+1}. <{tag_name}> {text[:50]}... (class: {class_name[:30]})")
                        except:
                            pass
                    
                    # ë‚´ìš© ë¶„ì„
                    print("\nğŸ” ë‚´ìš© ë¶„ì„:")
                    content_selectors = [
                        ".se-main-container",
                        ".se-component-content", 
                        ".se-text-paragraph",
                        ".article_content",
                        ".post_content",
                        ".content",
                        "[class*='content']",
                        "[class*='article']",
                        "[class*='post']",
                        "[class*='se-']"
                    ]
                    
                    for selector in content_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
                                for i, elem in enumerate(elements[:3]):
                                    try:
                                        text = elem.text.strip()
                                        if text and len(text) > 10:
                                            print(f"      {i+1}. {text[:50]}...")
                                    except:
                                        pass
                        except:
                            pass
                    
                    # ì‘ì„±ì ë¶„ì„
                    print("\nğŸ” ì‘ì„±ì ë¶„ì„:")
                    author_selectors = [
                        ".nick",
                        ".nickname",
                        ".author", 
                        ".writer",
                        "[class*='nick']",
                        "[class*='author']",
                        "[class*='writer']"
                    ]
                    
                    for selector in author_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
                                for i, elem in enumerate(elements[:3]):
                                    try:
                                        text = elem.text.strip()
                                        if text:
                                            print(f"      {i+1}. {text[:30]}...")
                                    except:
                                        pass
                        except:
                            pass
                    
                    # ëŒ“ê¸€ ë¶„ì„
                    print("\nğŸ” ëŒ“ê¸€ ë¶„ì„:")
                    comment_selectors = [
                        ".comment",
                        ".reply",
                        "[class*='comment']",
                        "[class*='reply']"
                    ]
                    
                    for selector in comment_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                print(f"   {selector}: {len(elements)}ê°œ ë°œê²¬")
                                for i, elem in enumerate(elements[:3]):
                                    try:
                                        text = elem.text.strip()
                                        if text and len(text) > 5:
                                            print(f"      {i+1}. {text[:30]}...")
                                    except:
                                        pass
                        except:
                            pass
                    
                    # ëª¨ë“  í´ë˜ìŠ¤ ì´ë¦„ ìˆ˜ì§‘
                    print("\nğŸ” ëª¨ë“  í´ë˜ìŠ¤ ë¶„ì„:")
                    all_elements = scraper.driver.find_elements("css selector", "*")
                    all_classes = set()
                    
                    for elem in all_elements:
                        try:
                            class_name = elem.get_attribute("class")
                            if class_name:
                                all_classes.update(class_name.split())
                        except:
                            continue
                    
                    # ê´€ë ¨ í‚¤ì›Œë“œê°€ í¬í•¨ëœ í´ë˜ìŠ¤ë“¤ë§Œ í•„í„°ë§
                    relevant_classes = []
                    keywords = ['title', 'content', 'article', 'post', 'author', 'nick', 'comment', 'reply', 'se-', 'text']
                    
                    for class_name in all_classes:
                        if any(keyword in class_name.lower() for keyword in keywords):
                            relevant_classes.append(class_name)
                    
                    print(f"   ê´€ë ¨ í´ë˜ìŠ¤ {len(relevant_classes)}ê°œ ë°œê²¬:")
                    for i, class_name in enumerate(sorted(relevant_classes)[:30]):  # ì²˜ìŒ 30ê°œë§Œ ì¶œë ¥
                        print(f"   {i+1}. {class_name}")
                    
                    print("\nâœ… ê²Œì‹œê¸€ ë¶„ì„ ì™„ë£Œ!")
                    break
                else:
                    print("âŒ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    analyze_direct_article()
