#!/usr/bin/env python3
"""
ì—…ë°ì´íŠ¸ëœ ì…€ë ‰í„°ë¡œ ë‹¨ì¼ ê²Œì‹œê¸€ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸
"""

import sys
import os
import time
import json
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from app.scraper.naver import NaverScraper

def test_updated_selectors():
    """ì—…ë°ì´íŠ¸ëœ ì…€ë ‰í„°ë¡œ ìŠ¤í¬ë˜í•‘ í…ŒìŠ¤íŠ¸"""
    
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # ë¸Œë¼ìš°ì € ì‹œì‘
        print("ğŸ”„ ë¸Œë¼ìš°ì € ì‹œì‘ ì¤‘...")
        scraper.start_browser()
        
        # ì¿ í‚¤ ë¡œë“œ
        scraper._load_cookies()
        
        # ì¤‘ê³ ë‚˜ë¼ ì¹´í˜ì˜ ì‹¤ì œ ê²Œì‹œê¸€ URL ì‹œë„
        test_urls = [
            "https://cafe.naver.com/joonggonara/1",  # ì¤‘ê³ ë‚˜ë¼ ê²Œì‹œíŒ
            "https://cafe.naver.com/steamindiegame/1",  # ìŠ¤íŒ€ ì¸ë””ê²Œì„ ê²Œì‹œíŒ
        ]
        
        for url in test_urls:
            print(f"\nğŸŒ ê²Œì‹œíŒ ì ‘ì† ì‹œë„: {url}")
            try:
                scraper.driver.get(url)
                time.sleep(5)  # ê¸°ë³¸ ë¡œë”© ëŒ€ê¸°
                
                # JavaScript ë¡œë”© ëŒ€ê¸°
                print("â³ JavaScript ë¡œë”© ëŒ€ê¸° ì¤‘... (30ì´ˆ)")
                time.sleep(30)
                
                # ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°
                print("ğŸ” ê²Œì‹œê¸€ ë§í¬ ì°¾ê¸°...")
                article_selectors = [
                    "a[href*='ArticleRead']",
                    "a[href*='ArticleRead.nhn']",
                    ".article a",
                    ".board_list a",
                    "tr td a",
                    "a[href*='cafe.naver.com']"
                ]
                
                article_links = []
                for selector in article_selectors:
                    try:
                        links = scraper.driver.find_elements("css selector", selector)
                        if links:
                            article_links = links
                            print(f"   {selector}: {len(links)}ê°œ ë°œê²¬")
                            break
                    except:
                        continue
                
                if article_links:
                    print(f"âœ… ê²Œì‹œê¸€ ë§í¬ {len(article_links)}ê°œ ë°œê²¬!")
                    
                    # ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ë§í¬ í´ë¦­
                    first_link = article_links[0]
                    article_url = first_link.get_attribute("href")
                    print(f"ğŸ“„ ì²« ë²ˆì§¸ ê²Œì‹œê¸€ ì ‘ì†: {article_url}")
                    
                    # ì—…ë°ì´íŠ¸ëœ ìŠ¤í¬ë˜í•‘ ë¡œì§ í…ŒìŠ¤íŠ¸
                    print("\nğŸ§ª ì—…ë°ì´íŠ¸ëœ ìŠ¤í¬ë˜í•‘ ë¡œì§ í…ŒìŠ¤íŠ¸:")
                    
                    # ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    print("ğŸ” ì œëª© ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
                    title_selectors = [
                        "h3.title",
                        "h2.title", 
                        "h1.title",
                        ".title",
                        ".se-title-text",
                        ".se-fs-",
                        ".se-component-content h1",
                        ".se-component-content h2", 
                        ".se-component-content h3",
                        ".se-text-paragraph"
                    ]
                    
                    title_found = False
                    for selector in title_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                for elem in elements:
                                    text = elem.text.strip()
                                    if text and len(text) > 5:
                                        print(f"   âœ… {selector}: {text[:50]}...")
                                        title_found = True
                                        break
                                if title_found:
                                    break
                        except:
                            continue
                    
                    if not title_found:
                        print("   âŒ ì œëª©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
                    # ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    print("\nğŸ” ë‚´ìš© ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
                    content_selectors = [
                        ".content",
                        ".se-main-container",
                        ".se-component-content",
                        ".se-text-paragraph",
                        ".se-text",
                        ".se-component",
                        ".article_content",
                        ".post_content"
                    ]
                    
                    content_found = False
                    for selector in content_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                for elem in elements:
                                    text = elem.text.strip()
                                    if text and len(text) > 10:
                                        print(f"   âœ… {selector}: {text[:50]}...")
                                        content_found = True
                                        break
                                if content_found:
                                    break
                        except:
                            continue
                    
                    if not content_found:
                        print("   âŒ ë‚´ìš©ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
                    # ì‘ì„±ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    print("\nğŸ” ì‘ì„±ì ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
                    author_selectors = [
                        ".nick",
                        ".nickname",
                        ".author", 
                        ".writer",
                        "[class*='nick']",
                        "[class*='author']",
                        "[class*='writer']"
                    ]
                    
                    author_found = False
                    for selector in author_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                for elem in elements:
                                    text = elem.text.strip()
                                    if text:
                                        print(f"   âœ… {selector}: {text[:30]}...")
                                        author_found = True
                                        break
                                if author_found:
                                    break
                        except:
                            continue
                    
                    if not author_found:
                        print("   âŒ ì‘ì„±ìë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
                    # ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸
                    print("\nğŸ” ëŒ“ê¸€ ì¶”ì¶œ í…ŒìŠ¤íŠ¸:")
                    comment_selectors = [
                        ".comment_area",
                        ".LinkComment",
                        ".comment",
                        ".reply",
                        "[class*='comment']",
                        "[class*='reply']"
                    ]
                    
                    comment_found = False
                    for selector in comment_selectors:
                        try:
                            elements = scraper.driver.find_elements("css selector", selector)
                            if elements:
                                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                                comment_found = True
                                break
                        except:
                            continue
                    
                    if not comment_found:
                        print("   âŒ ëŒ“ê¸€ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
                    print("\nâœ… í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
                    break
                else:
                    print("âŒ ê²Œì‹œê¸€ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                    
            except Exception as e:
                print(f"âŒ ì˜¤ë¥˜: {e}")
                continue
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        try:
            scraper.close()
        except:
            pass

if __name__ == "__main__":
    test_updated_selectors()
