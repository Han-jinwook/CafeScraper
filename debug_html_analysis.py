#!/usr/bin/env python3
"""
ë„¤ì´ë²„ ì¹´í˜ HTML êµ¬ì¡° ë¶„ì„ ë„êµ¬
ì‹¤ì œ í˜ì´ì§€ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ ì˜¬ë°”ë¥¸ ì…€ë ‰í„°ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
"""

import os
import time
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import json

def analyze_naver_cafe_structure():
    """ë„¤ì´ë²„ ì¹´í˜ HTML êµ¬ì¡°ë¥¼ ìƒì„¸ ë¶„ì„"""
    
    # Chrome ì˜µì…˜ ì„¤ì •
    chrome_options = Options()
    chrome_options.add_argument("--no-sandbox")
    chrome_options.add_argument("--disable-dev-shm-usage")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
    chrome_options.add_experimental_option('useAutomationExtension', False)
    
    # User-Agent ì„¤ì •
    chrome_options.add_argument("--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36")
    
    try:
        # WebDriver ì´ˆê¸°í™”
        service = Service(ChromeDriverManager().install())
        driver = webdriver.Chrome(service=service, options=chrome_options)
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
        
        print("ğŸ” ë„¤ì´ë²„ ì¹´í˜ HTML êµ¬ì¡° ë¶„ì„ ì‹œì‘...")
        
        # í…ŒìŠ¤íŠ¸í•  ê²Œì‹œê¸€ URL (ì‹¤ì œ ê²Œì‹œê¸€)
        test_urls = [
            "https://cafe.naver.com/f-e/cafes/27870803/articles/66575?boardtype=L&menuid=185&referrerAllArticles=false",
            "https://cafe.naver.com/f-e/cafes/27870803/articles/66747?boardtype=L&menuid=185&referrerAllArticles=false",
            "https://cafe.naver.com/f-e/cafes/27870803/articles/66743?boardtype=L&menuid=185&referrerAllArticles=false"
        ]
        
        for i, url in enumerate(test_urls, 1):
            print(f"\nğŸ“„ ê²Œì‹œê¸€ {i} ë¶„ì„: {url}")
            
            try:
                # í˜ì´ì§€ ë¡œë“œ
                driver.get(url)
                time.sleep(5)  # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                
                # ìŠ¤í¬ë¦°ìƒ· ì €ì¥
                driver.save_screenshot(f"debug_article_{i}.png")
                
                # í˜ì´ì§€ ì†ŒìŠ¤ ë¶„ì„
                page_source = driver.page_source
                
                # BeautifulSoupìœ¼ë¡œ íŒŒì‹±
                soup = BeautifulSoup(page_source, 'html.parser')
                
                print(f"âœ… í˜ì´ì§€ ë¡œë“œ ì™„ë£Œ: {url}")
                
                # 1. ì œëª© ë¶„ì„
                print("\nğŸ” ì œëª© ë¶„ì„:")
                analyze_title_structure(driver, soup, i)
                
                # 2. ë‚´ìš© ë¶„ì„
                print("\nğŸ” ë‚´ìš© ë¶„ì„:")
                analyze_content_structure(driver, soup, i)
                
                # 3. ì‘ì„±ì ë¶„ì„
                print("\nğŸ” ì‘ì„±ì ë¶„ì„:")
                analyze_author_structure(driver, soup, i)
                
                # 4. ëŒ“ê¸€ ë¶„ì„
                print("\nğŸ” ëŒ“ê¸€ ë¶„ì„:")
                analyze_comment_structure(driver, soup, i)
                
            except Exception as e:
                print(f"âŒ ê²Œì‹œê¸€ {i} ë¶„ì„ ì‹¤íŒ¨: {e}")
                continue
        
        driver.quit()
        print("\nâœ… HTML êµ¬ì¡° ë¶„ì„ ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë¶„ì„ ì‹¤íŒ¨: {e}")

def analyze_title_structure(driver, soup, article_num):
    """ì œëª© êµ¬ì¡° ë¶„ì„"""
    
    # 1. ëª¨ë“  h íƒœê·¸ ì°¾ê¸°
    h_tags = soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6'])
    print(f"   ğŸ“‹ ë°œê²¬ëœ h íƒœê·¸: {len(h_tags)}ê°œ")
    
    for i, h_tag in enumerate(h_tags[:5]):  # ì²˜ìŒ 5ê°œë§Œ ì¶œë ¥
        text = h_tag.get_text().strip()
        classes = h_tag.get('class', [])
        if text:
            print(f"     h{i+1}: '{text[:50]}...' (í´ë˜ìŠ¤: {classes})")
    
    # 2. title ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸°
    title_elements = soup.find_all(attrs={'class': lambda x: x and 'title' in ' '.join(x).lower()})
    print(f"   ğŸ“‹ title ê´€ë ¨ í´ë˜ìŠ¤: {len(title_elements)}ê°œ")
    
    for i, elem in enumerate(title_elements[:3]):
        text = elem.get_text().strip()
        classes = elem.get('class', [])
        if text:
            print(f"     title {i+1}: '{text[:50]}...' (í´ë˜ìŠ¤: {classes})")
    
    # 3. se- ê´€ë ¨ í´ë˜ìŠ¤ ì°¾ê¸° (ë„¤ì´ë²„ ì—ë””í„°)
    se_elements = soup.find_all(attrs={'class': lambda x: x and any('se-' in cls for cls in x)})
    print(f"   ğŸ“‹ se- ê´€ë ¨ í´ë˜ìŠ¤: {len(se_elements)}ê°œ")
    
    for i, elem in enumerate(se_elements[:3]):
        text = elem.get_text().strip()
        classes = elem.get('class', [])
        if text:
            print(f"     se- {i+1}: '{text[:50]}...' (í´ë˜ìŠ¤: {classes})")
    
    # 4. ì‹¤ì œ ê²Œì‹œê¸€ ì œëª©ì´ ìˆì„ ìˆ˜ ìˆëŠ” ì˜ì—­ë“¤ í™•ì¸
    print("   ğŸ” ê²Œì‹œê¸€ ì œëª© í›„ë³´ ì˜ì—­:")
    
    # ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ ì°¾ê¸°
    content_areas = [
        '.se-main-container',
        '.se-component-content', 
        '.article_content',
        '.post_content',
        '.content',
        '[class*="article"]',
        '[class*="content"]'
    ]
    
    for selector in content_areas:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"     âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for j, elem in enumerate(elements[:2]):
                    try:
                        text = elem.text.strip()
                        if text and len(text) > 10:  # ì˜ë¯¸ìˆëŠ” í…ìŠ¤íŠ¸ë§Œ
                            print(f"       ë‚´ìš© {j+1}: '{text[:100]}...'")
                    except:
                        pass
        except:
            pass

def analyze_content_structure(driver, soup, article_num):
    """ë‚´ìš© êµ¬ì¡° ë¶„ì„"""
    
    # 1. se- ê´€ë ¨ í´ë˜ìŠ¤ë“¤ (ë„¤ì´ë²„ ì—ë””í„°)
    se_elements = soup.find_all(attrs={'class': lambda x: x and any('se-' in cls for cls in x)})
    print(f"   ğŸ“‹ se- ê´€ë ¨ ìš”ì†Œ: {len(se_elements)}ê°œ")
    
    # 2. ê²Œì‹œê¸€ ë³¸ë¬¸ ì˜ì—­ë“¤
    content_selectors = [
        '.se-main-container',
        '.se-component-content',
        '.article_content',
        '.post_content',
        '.content',
        '[class*="content"]',
        '[class*="article"]'
    ]
    
    for selector in content_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:2]):
                    try:
                        text = elem.text.strip()
                        if text and len(text) > 20:
                            print(f"     ë‚´ìš© {i+1}: '{text[:100]}...'")
                    except:
                        pass
        except:
            pass

def analyze_author_structure(driver, soup, article_num):
    """ì‘ì„±ì êµ¬ì¡° ë¶„ì„"""
    
    # ì‘ì„±ì ê´€ë ¨ í´ë˜ìŠ¤ë“¤
    author_selectors = [
        '.nick',
        '.nickname',
        '.author',
        '.writer',
        '[class*="nick"]',
        '[class*="author"]',
        '[class*="writer"]'
    ]
    
    for selector in author_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
                for i, elem in enumerate(elements[:3]):
                    try:
                        text = elem.text.strip()
                        if text:
                            print(f"     ì‘ì„±ì {i+1}: '{text}'")
                    except:
                        pass
        except:
            pass

def analyze_comment_structure(driver, soup, article_num):
    """ëŒ“ê¸€ êµ¬ì¡° ë¶„ì„"""
    
    # ëŒ“ê¸€ ê´€ë ¨ í´ë˜ìŠ¤ë“¤
    comment_selectors = [
        '.comment',
        '.reply',
        '[class*="comment"]',
        '[class*="reply"]'
    ]
    
    for selector in comment_selectors:
        try:
            elements = driver.find_elements(By.CSS_SELECTOR, selector)
            if elements:
                print(f"   âœ… {selector}: {len(elements)}ê°œ ë°œê²¬")
        except:
            pass

if __name__ == "__main__":
    analyze_naver_cafe_structure()
