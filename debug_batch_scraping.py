#!/usr/bin/env python3
"""
ë°°ì¹˜ í¬ë¡¤ë§ ë””ë²„ê¹… ìŠ¤í¬ë¦½íŠ¸
ì›¹ UIì™€ ë™ì¼í•œ ë°©ì‹ìœ¼ë¡œ ë°°ì¹˜ í¬ë¡¤ë§ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
"""

import sys
import os
import time
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
project_root = Path(__file__).parent
sys.path.insert(0, str(project_root))

from app.scraper.naver import NaverScraper

def debug_batch_scraping():
    """ë°°ì¹˜ í¬ë¡¤ë§ ë””ë²„ê¹…"""
    print("ğŸ” ë°°ì¹˜ í¬ë¡¤ë§ ë””ë²„ê¹… ì‹œì‘")
    print("=" * 50)
    
    # ìŠ¤í¬ë˜í¼ ì´ˆê¸°í™”
    scraper = NaverScraper("sessions", "snapshots")
    
    try:
        # 1. ë¸Œë¼ìš°ì € ì‹œì‘
        print("1ï¸âƒ£ ë¸Œë¼ìš°ì € ì‹œì‘")
        scraper.start_browser()
        
        # 2. ì¿ í‚¤ ë¡œë“œ
        print("2ï¸âƒ£ ì¿ í‚¤ ë¡œë“œ")
        scraper._load_cookies()
        
        # 3. ë°°ì¹˜ í¬ë¡¤ë§ íŒŒë¼ë¯¸í„° ì„¤ì •
        cafe_url = "https://cafe.naver.com/sundreamd"
        max_pages = 1
        all_boards = False
        selected_boards = ["185"]  # â˜…ê°€ì…ì¸ì‚¬
        search_keywords = ["ê±´ì„ "]
        post_authors = []
        comment_authors = []
        max_articles = 5
        image_processing = "base64"
        period = "all"
        delay_between_requests = 3
        
        print("3ï¸âƒ£ ë°°ì¹˜ í¬ë¡¤ë§ íŒŒë¼ë¯¸í„°")
        print(f"   ì¹´í˜ URL: {cafe_url}")
        print(f"   ìµœëŒ€ í˜ì´ì§€: {max_pages}")
        print(f"   ì „ì²´ ê²Œì‹œíŒ: {all_boards}")
        print(f"   ì„ íƒëœ ê²Œì‹œíŒ: {selected_boards}")
        print(f"   ê²€ìƒ‰ í‚¤ì›Œë“œ: {search_keywords}")
        print(f"   ìµœëŒ€ ê²Œì‹œê¸€: {max_articles}")
        
        # 4. ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰
        print("\n4ï¸âƒ£ ë°°ì¹˜ í¬ë¡¤ë§ ì‹¤í–‰")
        results = scraper.batch_scraping(
            cafe_url=cafe_url,
            max_pages=max_pages,
            all_boards=all_boards,
            selected_boards=selected_boards,
            search_keywords=search_keywords,
            post_authors=post_authors,
            comment_authors=comment_authors,
            max_articles=max_articles,
            image_processing=image_processing,
            period=period,
            delay_between_requests=delay_between_requests
        )
        
        print(f"\n5ï¸âƒ£ ê²°ê³¼ ë¶„ì„")
        print(f"   ì´ ê²°ê³¼ ìˆ˜: {len(results)}")
        
        if results:
            print("âœ… ìŠ¤í¬ë˜í•‘ ì„±ê³µ!")
            for i, result in enumerate(results[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ ì¶œë ¥
                print(f"   {i}. {result.get('title', 'N/A')[:50]}...")
        else:
            print("âŒ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ - ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")
        
        print("\n" + "=" * 50)
        print("ğŸ‰ ë°°ì¹˜ í¬ë¡¤ë§ ë””ë²„ê¹… ì™„ë£Œ")
        
    except Exception as e:
        print(f"âŒ ë””ë²„ê¹… ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
    
    finally:
        # ë¸Œë¼ìš°ì € ì¢…ë£Œ
        if scraper.driver:
            scraper.close()
            print("ğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œë¨")

if __name__ == "__main__":
    debug_batch_scraping()

